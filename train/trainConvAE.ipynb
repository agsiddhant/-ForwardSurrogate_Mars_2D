{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#from gaia import *\n",
    "import glob, os\n",
    "\n",
    "import numpy as np\n",
    "import IPython\n",
    "from ipywidgets import *\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)\n",
    "\n",
    "from keras.layers import Activation, Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Cropping2D, Cropping3D, Deconvolution2D, Flatten, Add, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard, LearningRateScheduler\n",
    "from keras.utils import multi_gpu_model \n",
    "from keras.callbacks import History \n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from IPython.display import HTML\n",
    "from itertools import chain\n",
    "import random\n",
    "\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "import pickle\n",
    "import re\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "\n",
    "#%matplotlib notebook\n",
    "#%matplotlib notebook \n",
    "%matplotlib inline\n",
    "from matplotlib import ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.tri as tri\n",
    "font = {'size'   : 18}\n",
    "matplotlib.rc('font', **font)\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import time\n",
    "import sklearn\n",
    "from time import sleep \n",
    "from sklearn.utils import shuffle\n",
    "from multiprocessing import Process, Queue\n",
    "import threading\n",
    "\n",
    "from my_classes import DataGenerator\n",
    "from my_classes import PatchedModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathSave = \"C:/Users/agsid/Google Drive/ML/\"\n",
    "pathSave = \"/zeus/zeususer/agar_si/gaia/python/Compressed_Forward_Mars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    def __init__(self,_data, _pathSave, _numChannels, _sVar):\n",
    "        self.pathSave = _pathSave\n",
    "        self.sVar = _sVar\n",
    "\n",
    "        self.x = _data.x\n",
    "        self.y = _data.y\n",
    "        \n",
    "        self.numChannels = _numChannels\n",
    "        self.numFolders_tr = 8048\n",
    "        self.numFolders_val = 441\n",
    "        self.xdim0 = 302\n",
    "        self.xdim1 = 394\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    def NN(self, _hSize, _NEPOCH, _learnRate, _mpDictTrain={}, _mpDictTest={}):\n",
    "        graph = tf.get_default_graph()\n",
    "        input_img = Input(shape=(self.xdim0, self.xdim1, self.numChannels))  \n",
    "        # adapt this if using `channels_first` image data format\n",
    "        l2reg = l2(1e-7)\n",
    "        activFunc = 'tanh'\n",
    "        fx = 5\n",
    "        fy = 5\n",
    "        sx = 3\n",
    "        sy = 3\n",
    "        x = Conv2D(3, (fx, fy), strides=(sx,sy), padding='same', activation=activFunc, kernel_regularizer=l2reg)(input_img)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(6, (fx, fy), strides=(sx,sy), padding='same', activation=activFunc, kernel_regularizer=l2reg)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(9, (fx, fy), strides=(sx,sy), padding='same', activation=activFunc, kernel_regularizer=l2reg)(x)\n",
    "        encoded = BatchNormalization()(x)\n",
    "        \n",
    "        x = Deconvolution2D(6, (fx, fy), strides=(sx,sy), padding='same', activation=activFunc, kernel_regularizer=l2reg)(encoded)\n",
    "        x = Cropping2D(cropping=((1, 1), (1, 0)), data_format=None)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = Deconvolution2D(3, (fx, fy), strides=(sx,sy), padding='same', activation=activFunc, kernel_regularizer=l2reg)(x)\n",
    "        x = Cropping2D(cropping=((1, 0), (0, 0)), data_format=None)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = Deconvolution2D(1, (fx, fy), strides=(sx,sy), padding='same', activation=activFunc, kernel_regularizer=l2reg)(x)\n",
    "        x = Cropping2D(cropping=((0, 1), (1, 1)), data_format=None)(x)   \n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        decoded = Conv2D(self.numChannels, (fx, fy), strides=(1,1), padding='same', activation=activFunc, kernel_regularizer=l2reg)(x)\n",
    "        \n",
    "        paramsTrain = {'trainOrVal': 'train',\n",
    "                       'batch_size': 64,\n",
    "                       'xdim': (self.xdim0,self.xdim1), \n",
    "                       'ydim': (self.xdim0,self.xdim1),\n",
    "                       'n_channels': self.numChannels,\n",
    "                       'shuffle': True, \n",
    "                       'isAutoencoder': True}\n",
    "                        \n",
    "        paramsVal = {'trainOrVal': 'cv',\n",
    "                       'batch_size': 64,\n",
    "                       'xdim': (self.xdim0,self.xdim1), \n",
    "                       'ydim': (self.xdim0,self.xdim1),\n",
    "                       'n_channels': self.numChannels,\n",
    "                       'shuffle': True, \n",
    "                       'isAutoencoder': True}\n",
    "        \n",
    "        validation_generator = DataGenerator(self.pathSave, self.sVar, self.numFolders_val, **paramsVal)\n",
    "        training_generator = DataGenerator(self.pathSave, self.sVar, self.numFolders_tr, **paramsTrain)\n",
    "        \n",
    "        class TimeHistory(keras.callbacks.Callback):\n",
    "            def on_train_begin(self, logs={}):\n",
    "                self.times = []\n",
    "\n",
    "            def on_epoch_begin(self, batch, logs={}):\n",
    "                self.epoch_time_start = time.time()\n",
    "\n",
    "            def on_epoch_end(self, batch, logs={}):\n",
    "                self.times.append(time.time() - self.epoch_time_start)\n",
    "        \n",
    "        class LossPrintingCallback(tf.keras.callbacks.Callback):\n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                if epoch%1000==0:\n",
    "                    print('learning rate is {:.16e}'.format(tf.keras.backend.get_value(self.model.optimizer.lr)))\n",
    "                    \n",
    "        def scheduler(epoch):\n",
    "            initial_lrate = _learnRate\n",
    "            if epoch < 1:\n",
    "                return initial_lrate\n",
    "            elif epoch >= 1 and epoch < 2:\n",
    "                return initial_lrate/10.\n",
    "            elif epoch >= 2 and epoch < 3:\n",
    "                return initial_lrate/100.\n",
    "            else:\n",
    "                return initial_lrate/1000.\n",
    "        \n",
    "        def root_mean_squared_error(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "        \n",
    "        config = tf.ConfigProto(\n",
    "        #device_count={'GPU': 1},\n",
    "        intra_op_parallelism_threads=1,\n",
    "        allow_soft_placement=True)\n",
    "\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "\n",
    "        session = tf.Session(config=config)\n",
    "        \n",
    "        GPUS = 1\n",
    "        name = self.sVar + '_Mars300_sims10k_f5s3_c369_tanh_l2reg_wlrsc'\n",
    "        \n",
    "        with session.as_default():\n",
    "            with session.graph.as_default():\n",
    "                if GPUS >= 2:\n",
    "                    with tf.device(\"/cpu:0\"):\n",
    "                        # initialize the model\n",
    "                        autoencoder = Model(input_img, decoded)\n",
    "\n",
    "                    # make the model parallel\n",
    "                    autoencoder = multi_gpu_model(autoencoder, gpus=GPUS)\n",
    "                else:\n",
    "                    autoencoder = Model(input_img, decoded)\n",
    "                    #autoencoder = load_model(self.pathSave + \"/TrainedNetworks/autoencoder/\" + name + \".hdf5\")\n",
    "                autoencoder.compile(optimizer=keras.optimizers.Adam(), loss='mse') #lr=_learnRate, amsgrad=True\n",
    "                autoencoder.summary()\n",
    "                \n",
    "                K.get_session().run(tf.global_variables_initializer())\n",
    "                \n",
    "                history = History()\n",
    "                cp = PatchedModelCheckpoint(self.pathSave + 'TrainedNetworks/autoencoder/' + name + '.{epoch:02d}-{val_loss:.5f}.hdf5', \n",
    "                monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "                time_callback = TimeHistory()\n",
    "                lr_scheduler = LearningRateScheduler(scheduler)\n",
    "                csvlogger = tf.keras.callbacks.CSVLogger(self.pathSave+'TrainedNetworks/autoencoder/'+name+'.txt', separator=\",\", append=True)\n",
    "                \n",
    "                ae = autoencoder.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=16,\n",
    "                    verbose=1,\n",
    "                    max_queue_size=100,                 \n",
    "                    epochs=100,\n",
    "                    callbacks=[cp, csvlogger, lr_scheduler]) #, TensorBoard(log_dir= self.pathSave + '/TrainedNetworks/autoencoder')])\n",
    "                \n",
    "                #with open(self.pathSave + 'TrainedNetworks/autoencoder/History_' + name + '.txt', 'wb') as file_pi:\n",
    "                #    dump(ae.history, file_pi)\n",
    "                #autoencoder.save(self.pathSave + 'TrainedNetworks/autoencoder/my_model.h5')\n",
    "        \n",
    "        return self "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1010 18:33:19.183356 139830479693632 module_wrapper.py:139] From /zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1010 18:33:19.185714 139830479693632 module_wrapper.py:139] From /zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1010 18:33:19.213753 139830479693632 module_wrapper.py:139] From /zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1010 18:33:19.214518 139830479693632 module_wrapper.py:139] From /zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1010 18:33:19.215050 139830479693632 module_wrapper.py:139] From /zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W1010 18:33:20.832025 139830479693632 module_wrapper.py:139] From /zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W1010 18:33:20.832993 139830479693632 module_wrapper.py:139] From /zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W1010 18:33:21.028043 139830479693632 module_wrapper.py:139] From /zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "W1010 18:33:32.290331 139830479693632 module_wrapper.py:139] From /zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W1010 18:33:32.348433 139830479693632 module_wrapper.py:139] From /zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1010 18:33:33.535064 139830479693632 module_wrapper.py:139] From /zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 302, 394, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 101, 132, 3)       78        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 101, 132, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 44, 6)         456       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 34, 44, 6)         24        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 15, 9)         1359      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 15, 9)         36        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 36, 45, 6)         1356      \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 34, 44, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 34, 44, 6)         24        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 102, 132, 3)       453       \n",
      "_________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)    (None, 101, 132, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 101, 132, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 303, 396, 1)       76        \n",
      "_________________________________________________________________\n",
      "cropping2d_3 (Cropping2D)    (None, 302, 394, 1)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 302, 394, 1)       4         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 302, 394, 1)       26        \n",
      "=================================================================\n",
      "Total params: 3,916\n",
      "Trainable params: 3,860\n",
      "Non-trainable params: 56\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1010 18:33:33.858285 139830479693632 module_wrapper.py:139] From /zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W1010 18:33:34.023474 139830479693632 module_wrapper.py:139] From /zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "2 root error(s) found.\n  (0) Failed precondition: Attempting to use uninitialized value conv2d_1/bias\n\t [[{{node conv2d_1/bias/read}}]]\n\t [[loss/add_6/_327]]\n  (1) Failed precondition: Attempting to use uninitialized value conv2d_1/bias\n\t [[{{node conv2d_1/bias/read}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5cec85bf45e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mR1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathSave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumChannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msVar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mR1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Architecture\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#resultsDict[\"loss_train\"] = results.Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4d56a6a81567>\u001b[0m in \u001b[0;36mNN\u001b[0;34m(self, _hSize, _NEPOCH, _learnRate, _mpDictTrain, _mpDictTest)\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                     callbacks=[cp, csvlogger, lr_scheduler]) #, TensorBoard(log_dir= self.pathSave + '/TrainedNetworks/autoencoder')])\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;31m#with open(self.pathSave + 'TrainedNetworks/autoencoder/History_' + name + '.txt', 'wb') as file_pi:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/zeus/zeus_scr/agar_si/python/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: 2 root error(s) found.\n  (0) Failed precondition: Attempting to use uninitialized value conv2d_1/bias\n\t [[{{node conv2d_1/bias/read}}]]\n\t [[loss/add_6/_327]]\n  (1) Failed precondition: Attempting to use uninitialized value conv2d_1/bias\n\t [[{{node conv2d_1/bias/read}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "## NN parameters\n",
    "nnDict = {}\n",
    "counter = 0\n",
    "Variables = ['T']\n",
    "nnDict[\"Architecture\" + str(counter)] = [1000,500,1000]\n",
    "counter += 1\n",
    "nnCounter = counter\n",
    "numChannels = 1\n",
    "\n",
    "resultsDict = {}\n",
    "for sVar in Variables:\n",
    "    dictToOpen = \"processedDicts/Mars_new/ConvAE/\" + sVar + \"/Dict_processed_data_MarsNew_2D_ConvAE_\" + sVar + \"_[12000]sims_1channel.txt\" \n",
    "    with open(pathSave + dictToOpen, \"rb\") as myFile:\n",
    "        Dict_processed_data = load(myFile)\n",
    "        \n",
    "    for i in range(0,nnCounter):\n",
    "\n",
    "        class createData():\n",
    "            pass\n",
    "        data = createData()\n",
    "        #data.x_data = Dict_processed_data_train #['x_data']\n",
    "        #data.x_cv = Dict_processed_data_cv #['x_cv']\n",
    "        data.x = Dict_processed_data['x'] \n",
    "        data.y = Dict_processed_data['y'] \n",
    "        \n",
    "        R1 = Regression(data, pathSave, numChannels, sVar)\n",
    "        results = R1.NN(nnDict[\"Architecture\" + str(i)], 5, 0.001)\n",
    "\n",
    "        #resultsDict[\"loss_train\"] = results.Loss\n",
    "        #resultsDict[\"loss_test\"]  = results.Loss_cv\n",
    "        #resultsDict[\"x_data\"]     = results.x_data\n",
    "        #resultsDict[\"x_test\"]     = results.x_test\n",
    "        #resultsDict[\"x_cv\"]       = results.x_cv\n",
    "\n",
    "        print()\n",
    "        #with open(pathSave + \"TrainedNetworks/\" + \"/resultsDict\" + str(nnDict[\"Architecture\" + str(i)]) + \".txt\", \"wb\") as fkl:\n",
    "        #    pickle.dump(resultsDict, fkl, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
