{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T16:15:10.769380Z",
     "iopub.status.busy": "2021-03-12T16:15:10.753520Z",
     "iopub.status.idle": "2021-03-12T16:15:16.622590Z",
     "shell.execute_reply": "2021-03-12T16:15:16.623136Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#from gaia import *\n",
    "import glob, os\n",
    "\n",
    "import numpy as np\n",
    "import IPython\n",
    "from ipywidgets import *\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import HTML\n",
    "from itertools import chain\n",
    "import random\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pickle\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Cropping2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import multi_gpu_model \n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "#%matplotlib notebook\n",
    "#%matplotlib notebook \n",
    "%matplotlib inline\n",
    "from matplotlib import ticker\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "font = {'family':'Times New Roman',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 20}\n",
    "matplotlib.rc('font', **font)\n",
    "from matplotlib import rcParams\n",
    "fontSize= 12\n",
    "params = {'axes.labelsize': fontSize,'axes.titlesize':fontSize, 'font.size': fontSize, \\\n",
    "          'legend.fontsize': 12, 'xtick.labelsize':fontSize, 'ytick.labelsize': fontSize}\n",
    "matplotlib.rcParams.update(params)\n",
    "matplotlib.rc('text', usetex=True)\n",
    "\n",
    "from decimal import Decimal\n",
    "from sklearn import manifold\n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "from joblib import load, dump, Parallel, delayed\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T16:15:16.626869Z",
     "iopub.status.busy": "2021-03-12T16:15:16.626225Z",
     "iopub.status.idle": "2021-03-12T16:15:16.658740Z",
     "shell.execute_reply": "2021-03-12T16:15:16.659207Z"
    }
   },
   "outputs": [],
   "source": [
    "pathSave = \"/plp_user/agar_si/gaia/python/Compressed_Forward_Mars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T16:15:16.671025Z",
     "iopub.status.busy": "2021-03-12T16:15:16.670428Z",
     "iopub.status.idle": "2021-03-12T16:15:16.705476Z",
     "shell.execute_reply": "2021-03-12T16:15:16.704771Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class getDataConvAE:\n",
    "    def __init__(self,_numSims, _pathSave, numChannels, sVar, indexerDict):\n",
    "        self.pathSave = _pathSave\n",
    "        self.numSims = _numSims  \n",
    "        self.sVar = sVar\n",
    "        self.indexerDict = indexerDict\n",
    "        \n",
    "        for s in [0, 1, 2]:\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "                \n",
    "            pathSaveE = pathSave + \"processedDicts/Mars_new/ConvAE/\" + sVar + \"/train\" \n",
    "            if not os.path.exists(pathSaveE):\n",
    "                os.makedirs(pathSaveE)\n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "            \n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "            \n",
    "            simList = []\n",
    "            timeList = []\n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    if dictInd != -1:\n",
    "                        timeList.append(innertimeList)\n",
    "                        simList.append(dictInd)\n",
    "                    dictInd = val[0]\n",
    "                    innertimeList = [val[2]]\n",
    "                else:\n",
    "                    innertimeList.append(val[2])    \n",
    "                    \n",
    "            simList.append(dictInd)\n",
    "            timeList.append(innertimeList)\n",
    "\n",
    "            numStateVariables = numChannels #T, x, y\n",
    "            sizeStateVariable = self.indexerDict[\"x\"].shape[0] \n",
    "\n",
    "            numLayers = 302 #202\n",
    "            sizeEachLayer = int(sizeStateVariable/numLayers)\n",
    "            \n",
    "            totalSamples = len(indexerVec)\n",
    "            numBatches = len(simList) #int(np.ceil(totalSamples/self.batchSize)) \n",
    "            \n",
    "            print(len(simList),len(timeList), totalSamples)  \n",
    "                            \n",
    "            def processEach(simInd):\n",
    "                #print(\"------------\" + str(batchN) + \"----------------\")\n",
    "                if not os.path.exists(pathSaveE + \"/\" + str(simList[simInd])):\n",
    "                    os.makedirs(pathSaveE + \"/\" + str(simList[simInd]))\n",
    "\n",
    "                dictToOpen = \"/plp_user/agar_si/gaia/python/Data/2D/Mars_new/\" + sVar + \"/savedDict_profiles_Mars_new_2D_\" \\\n",
    "                                                                            + sVar + \"_only_Dict\" + str(simList[simInd]) + \".txt\"\n",
    "                with open(dictToOpen, \"rb\") as myFile:\n",
    "                    profiles = load(myFile)\n",
    "                paras = [np.log10(float(profiles['InputValues0'][1].split('=')[1])), \\\n",
    "                                  float(profiles['InputValues0'][5].split('=')[1]), \\\n",
    "                                  float(profiles['InputValues0'][6].split('=')[1]), \\\n",
    "                                  float(profiles['InputValues0'][7].split('=')[1]), \\\n",
    "                                  float(profiles['InputValues0'][8].split('=')[1].split(\"_\")[0].split(\".ini\")[0])]\n",
    "                with open(pathSaveE + \"/\" + str(simList[simInd]) + \"/paras.txt\", \"wb\") as f:\n",
    "                    dump(paras , f, protocol=4)\n",
    "                    \n",
    "                #for t in timeList[simInd]:\n",
    "                #    field = profiles['Dict_2D_' + sVar + '_time_' + str(t) + '_' + str(0)]\n",
    "                #    field.shape = (1,numLayers,sizeEachLayer,numStateVariables)\n",
    "\n",
    "                    #field = (field-self.pMin)/(self.pMax-self.pMin)         \n",
    "                    #with open(pathSaveE + \"/\" + str(simList[simInd]) + \"/Dict_processed_data_MarsNew_2D_ConvAE_\" + str(t) + \".txt\", \"wb\") as f:\n",
    "                    #    dump(field , f, protocol=4)\n",
    "\n",
    "            Parallel(n_jobs=8, verbose=10)(delayed(processEach)(_bInd) for _bInd in range(len(simList)))\n",
    "        \n",
    "        profiles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T16:15:16.709904Z",
     "iopub.status.busy": "2021-03-12T16:15:16.709236Z",
     "iopub.status.idle": "2021-03-13T00:37:37.143104Z",
     "shell.execute_reply": "2021-03-13T00:37:37.142516Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9080 9080 958370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=8)]: Done  45 tasks      | elapsed:   29.3s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   30.0s\n",
      "[Parallel(n_jobs=8)]: Done  69 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=8)]: Done  82 tasks      | elapsed:   41.0s\n",
      "[Parallel(n_jobs=8)]: Done  97 tasks      | elapsed:   55.7s\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=8)]: Done 129 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=8)]: Done 165 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=8)]: Done 205 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=8)]: Done 226 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=8)]: Done 249 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=8)]: Done 297 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=8)]: Done 322 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=8)]: Done 376 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=8)]: Done 405 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=8)]: Done 465 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=8)]: Done 496 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=8)]: Done 529 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=8)]: Done 562 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=8)]: Done 597 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=8)]: Done 632 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=8)]: Done 669 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=8)]: Done 706 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=8)]: Done 745 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=8)]: Done 825 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=8)]: Done 866 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=8)]: Done 909 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=8)]: Done 952 tasks      | elapsed: 20.9min\n",
      "[Parallel(n_jobs=8)]: Done 997 tasks      | elapsed: 21.7min\n",
      "[Parallel(n_jobs=8)]: Done 1042 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=8)]: Done 1089 tasks      | elapsed: 23.2min\n",
      "[Parallel(n_jobs=8)]: Done 1136 tasks      | elapsed: 23.8min\n",
      "[Parallel(n_jobs=8)]: Done 1185 tasks      | elapsed: 24.6min\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed: 25.4min\n",
      "[Parallel(n_jobs=8)]: Done 1285 tasks      | elapsed: 26.2min\n",
      "[Parallel(n_jobs=8)]: Done 1336 tasks      | elapsed: 27.1min\n",
      "[Parallel(n_jobs=8)]: Done 1389 tasks      | elapsed: 27.9min\n",
      "[Parallel(n_jobs=8)]: Done 1442 tasks      | elapsed: 28.7min\n",
      "[Parallel(n_jobs=8)]: Done 1497 tasks      | elapsed: 29.5min\n",
      "[Parallel(n_jobs=8)]: Done 1552 tasks      | elapsed: 30.3min\n",
      "[Parallel(n_jobs=8)]: Done 1609 tasks      | elapsed: 31.2min\n",
      "[Parallel(n_jobs=8)]: Done 1666 tasks      | elapsed: 32.0min\n",
      "[Parallel(n_jobs=8)]: Done 1725 tasks      | elapsed: 32.9min\n",
      "[Parallel(n_jobs=8)]: Done 1784 tasks      | elapsed: 33.8min\n",
      "[Parallel(n_jobs=8)]: Done 1845 tasks      | elapsed: 34.8min\n",
      "[Parallel(n_jobs=8)]: Done 1906 tasks      | elapsed: 35.6min\n",
      "[Parallel(n_jobs=8)]: Done 1969 tasks      | elapsed: 36.6min\n",
      "[Parallel(n_jobs=8)]: Done 2032 tasks      | elapsed: 37.6min\n",
      "[Parallel(n_jobs=8)]: Done 2097 tasks      | elapsed: 38.6min\n",
      "[Parallel(n_jobs=8)]: Done 2162 tasks      | elapsed: 39.7min\n",
      "[Parallel(n_jobs=8)]: Done 2229 tasks      | elapsed: 40.8min\n",
      "[Parallel(n_jobs=8)]: Done 2296 tasks      | elapsed: 41.9min\n",
      "[Parallel(n_jobs=8)]: Done 2365 tasks      | elapsed: 43.2min\n",
      "[Parallel(n_jobs=8)]: Done 2434 tasks      | elapsed: 44.4min\n",
      "[Parallel(n_jobs=8)]: Done 2505 tasks      | elapsed: 45.7min\n",
      "[Parallel(n_jobs=8)]: Done 2576 tasks      | elapsed: 46.9min\n",
      "[Parallel(n_jobs=8)]: Done 2649 tasks      | elapsed: 48.1min\n",
      "[Parallel(n_jobs=8)]: Done 2722 tasks      | elapsed: 49.3min\n",
      "[Parallel(n_jobs=8)]: Done 2797 tasks      | elapsed: 50.7min\n",
      "[Parallel(n_jobs=8)]: Done 2872 tasks      | elapsed: 51.9min\n",
      "[Parallel(n_jobs=8)]: Done 2949 tasks      | elapsed: 53.1min\n",
      "[Parallel(n_jobs=8)]: Done 3026 tasks      | elapsed: 54.1min\n",
      "[Parallel(n_jobs=8)]: Done 3105 tasks      | elapsed: 55.5min\n",
      "[Parallel(n_jobs=8)]: Done 3184 tasks      | elapsed: 56.7min\n",
      "[Parallel(n_jobs=8)]: Done 3265 tasks      | elapsed: 57.8min\n",
      "[Parallel(n_jobs=8)]: Done 3346 tasks      | elapsed: 59.1min\n",
      "[Parallel(n_jobs=8)]: Done 3429 tasks      | elapsed: 60.4min\n",
      "[Parallel(n_jobs=8)]: Done 3512 tasks      | elapsed: 61.7min\n",
      "[Parallel(n_jobs=8)]: Done 3597 tasks      | elapsed: 63.0min\n",
      "[Parallel(n_jobs=8)]: Done 3682 tasks      | elapsed: 64.3min\n",
      "[Parallel(n_jobs=8)]: Done 3769 tasks      | elapsed: 65.8min\n",
      "[Parallel(n_jobs=8)]: Done 3856 tasks      | elapsed: 67.2min\n",
      "[Parallel(n_jobs=8)]: Done 3945 tasks      | elapsed: 68.6min\n",
      "[Parallel(n_jobs=8)]: Done 4034 tasks      | elapsed: 70.3min\n",
      "[Parallel(n_jobs=8)]: Done 4125 tasks      | elapsed: 72.9min\n",
      "[Parallel(n_jobs=8)]: Done 4216 tasks      | elapsed: 75.5min\n",
      "[Parallel(n_jobs=8)]: Done 4309 tasks      | elapsed: 78.5min\n",
      "[Parallel(n_jobs=8)]: Done 4402 tasks      | elapsed: 80.5min\n",
      "[Parallel(n_jobs=8)]: Done 4497 tasks      | elapsed: 81.9min\n",
      "[Parallel(n_jobs=8)]: Done 4592 tasks      | elapsed: 83.3min\n",
      "[Parallel(n_jobs=8)]: Done 4689 tasks      | elapsed: 84.7min\n",
      "[Parallel(n_jobs=8)]: Done 4786 tasks      | elapsed: 86.2min\n",
      "[Parallel(n_jobs=8)]: Done 4885 tasks      | elapsed: 87.7min\n",
      "[Parallel(n_jobs=8)]: Done 4984 tasks      | elapsed: 89.2min\n",
      "[Parallel(n_jobs=8)]: Done 5085 tasks      | elapsed: 90.6min\n",
      "[Parallel(n_jobs=8)]: Done 5186 tasks      | elapsed: 92.2min\n",
      "[Parallel(n_jobs=8)]: Done 5289 tasks      | elapsed: 93.8min\n",
      "[Parallel(n_jobs=8)]: Done 5392 tasks      | elapsed: 95.4min\n",
      "[Parallel(n_jobs=8)]: Done 5497 tasks      | elapsed: 97.1min\n",
      "[Parallel(n_jobs=8)]: Done 5602 tasks      | elapsed: 98.7min\n",
      "[Parallel(n_jobs=8)]: Done 5709 tasks      | elapsed: 100.3min\n",
      "[Parallel(n_jobs=8)]: Done 5816 tasks      | elapsed: 102.0min\n",
      "[Parallel(n_jobs=8)]: Done 5925 tasks      | elapsed: 103.6min\n",
      "[Parallel(n_jobs=8)]: Done 6034 tasks      | elapsed: 105.3min\n",
      "[Parallel(n_jobs=8)]: Done 6145 tasks      | elapsed: 106.9min\n",
      "[Parallel(n_jobs=8)]: Done 6256 tasks      | elapsed: 108.6min\n",
      "[Parallel(n_jobs=8)]: Done 6369 tasks      | elapsed: 110.5min\n",
      "[Parallel(n_jobs=8)]: Done 6482 tasks      | elapsed: 112.2min\n",
      "[Parallel(n_jobs=8)]: Done 6597 tasks      | elapsed: 114.0min\n",
      "[Parallel(n_jobs=8)]: Done 6712 tasks      | elapsed: 115.8min\n",
      "[Parallel(n_jobs=8)]: Done 6829 tasks      | elapsed: 117.4min\n",
      "[Parallel(n_jobs=8)]: Done 6946 tasks      | elapsed: 119.3min\n",
      "[Parallel(n_jobs=8)]: Done 7065 tasks      | elapsed: 121.0min\n",
      "[Parallel(n_jobs=8)]: Done 7184 tasks      | elapsed: 122.8min\n",
      "[Parallel(n_jobs=8)]: Done 7305 tasks      | elapsed: 124.6min\n",
      "[Parallel(n_jobs=8)]: Done 7426 tasks      | elapsed: 126.3min\n",
      "[Parallel(n_jobs=8)]: Done 7549 tasks      | elapsed: 128.2min\n",
      "[Parallel(n_jobs=8)]: Done 7672 tasks      | elapsed: 130.1min\n",
      "[Parallel(n_jobs=8)]: Done 7797 tasks      | elapsed: 132.1min\n",
      "[Parallel(n_jobs=8)]: Done 7922 tasks      | elapsed: 133.9min\n",
      "[Parallel(n_jobs=8)]: Done 8049 tasks      | elapsed: 135.8min\n",
      "[Parallel(n_jobs=8)]: Done 8176 tasks      | elapsed: 137.9min\n",
      "[Parallel(n_jobs=8)]: Done 8305 tasks      | elapsed: 139.8min\n",
      "[Parallel(n_jobs=8)]: Done 8434 tasks      | elapsed: 143.3min\n",
      "[Parallel(n_jobs=8)]: Done 8565 tasks      | elapsed: 147.2min\n",
      "[Parallel(n_jobs=8)]: Done 8696 tasks      | elapsed: 150.4min\n",
      "[Parallel(n_jobs=8)]: Done 8829 tasks      | elapsed: 152.4min\n",
      "[Parallel(n_jobs=8)]: Done 8962 tasks      | elapsed: 154.5min\n",
      "[Parallel(n_jobs=8)]: Done 9080 out of 9080 | elapsed: 156.3min finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501 501 51174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   27.6s\n",
      "[Parallel(n_jobs=8)]: Done  45 tasks      | elapsed:   38.3s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   46.2s\n",
      "[Parallel(n_jobs=8)]: Done  69 tasks      | elapsed:   57.0s\n",
      "[Parallel(n_jobs=8)]: Done  82 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=8)]: Done  97 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=8)]: Done 129 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=8)]: Done 165 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=8)]: Done 205 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=8)]: Done 226 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=8)]: Done 249 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=8)]: Done 297 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=8)]: Done 322 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=8)]: Done 376 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=8)]: Done 405 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=8)]: Done 465 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=8)]: Done 501 out of 501 | elapsed:  7.4min finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 499 50835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   33.3s\n",
      "[Parallel(n_jobs=8)]: Done  45 tasks      | elapsed:   44.7s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   53.8s\n",
      "[Parallel(n_jobs=8)]: Done  69 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=8)]: Done  82 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=8)]: Done  97 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=8)]: Done 129 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=8)]: Done 165 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=8)]: Done 205 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=8)]: Done 226 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=8)]: Done 249 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=8)]: Done 297 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=8)]: Done 322 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=8)]: Done 376 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=8)]: Done 405 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=8)]: Done 465 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=8)]: Done 499 out of 499 | elapsed:  7.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9080 9080 958370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   19.6s\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=8)]: Done  45 tasks      | elapsed:   45.6s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   58.4s\n",
      "[Parallel(n_jobs=8)]: Done  69 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=8)]: Done  82 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=8)]: Done  97 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=8)]: Done 129 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=8)]: Done 165 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=8)]: Done 205 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=8)]: Done 226 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=8)]: Done 249 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=8)]: Done 297 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=8)]: Done 322 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=8)]: Done 376 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=8)]: Done 405 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=8)]: Done 465 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=8)]: Done 496 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=8)]: Done 529 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=8)]: Done 562 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=8)]: Done 597 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=8)]: Done 632 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=8)]: Done 669 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=8)]: Done 706 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=8)]: Done 745 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=8)]: Done 825 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=8)]: Done 866 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=8)]: Done 909 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=8)]: Done 952 tasks      | elapsed: 15.3min\n",
      "[Parallel(n_jobs=8)]: Done 997 tasks      | elapsed: 16.0min\n",
      "[Parallel(n_jobs=8)]: Done 1042 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=8)]: Done 1089 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=8)]: Done 1136 tasks      | elapsed: 18.1min\n",
      "[Parallel(n_jobs=8)]: Done 1185 tasks      | elapsed: 18.8min\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=8)]: Done 1285 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=8)]: Done 1336 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=8)]: Done 1389 tasks      | elapsed: 22.1min\n",
      "[Parallel(n_jobs=8)]: Done 1442 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=8)]: Done 1497 tasks      | elapsed: 23.7min\n",
      "[Parallel(n_jobs=8)]: Done 1552 tasks      | elapsed: 24.5min\n",
      "[Parallel(n_jobs=8)]: Done 1609 tasks      | elapsed: 25.4min\n",
      "[Parallel(n_jobs=8)]: Done 1666 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=8)]: Done 1725 tasks      | elapsed: 27.0min\n",
      "[Parallel(n_jobs=8)]: Done 1784 tasks      | elapsed: 27.9min\n",
      "[Parallel(n_jobs=8)]: Done 1845 tasks      | elapsed: 28.9min\n",
      "[Parallel(n_jobs=8)]: Done 1906 tasks      | elapsed: 29.7min\n",
      "[Parallel(n_jobs=8)]: Done 1969 tasks      | elapsed: 30.7min\n",
      "[Parallel(n_jobs=8)]: Done 2032 tasks      | elapsed: 31.7min\n",
      "[Parallel(n_jobs=8)]: Done 2097 tasks      | elapsed: 32.6min\n",
      "[Parallel(n_jobs=8)]: Done 2162 tasks      | elapsed: 33.7min\n",
      "[Parallel(n_jobs=8)]: Done 2229 tasks      | elapsed: 34.8min\n",
      "[Parallel(n_jobs=8)]: Done 2296 tasks      | elapsed: 35.7min\n",
      "[Parallel(n_jobs=8)]: Done 2365 tasks      | elapsed: 36.9min\n",
      "[Parallel(n_jobs=8)]: Done 2434 tasks      | elapsed: 39.0min\n",
      "[Parallel(n_jobs=8)]: Done 2505 tasks      | elapsed: 41.1min\n",
      "[Parallel(n_jobs=8)]: Done 2576 tasks      | elapsed: 43.0min\n",
      "[Parallel(n_jobs=8)]: Done 2649 tasks      | elapsed: 45.0min\n",
      "[Parallel(n_jobs=8)]: Done 2722 tasks      | elapsed: 46.9min\n",
      "[Parallel(n_jobs=8)]: Done 2797 tasks      | elapsed: 48.2min\n",
      "[Parallel(n_jobs=8)]: Done 2872 tasks      | elapsed: 49.3min\n",
      "[Parallel(n_jobs=8)]: Done 2949 tasks      | elapsed: 50.5min\n",
      "[Parallel(n_jobs=8)]: Done 3026 tasks      | elapsed: 51.6min\n",
      "[Parallel(n_jobs=8)]: Done 3105 tasks      | elapsed: 52.8min\n",
      "[Parallel(n_jobs=8)]: Done 3184 tasks      | elapsed: 54.1min\n",
      "[Parallel(n_jobs=8)]: Done 3265 tasks      | elapsed: 55.3min\n",
      "[Parallel(n_jobs=8)]: Done 3346 tasks      | elapsed: 56.6min\n",
      "[Parallel(n_jobs=8)]: Done 3429 tasks      | elapsed: 57.8min\n",
      "[Parallel(n_jobs=8)]: Done 3512 tasks      | elapsed: 59.1min\n",
      "[Parallel(n_jobs=8)]: Done 3597 tasks      | elapsed: 60.3min\n",
      "[Parallel(n_jobs=8)]: Done 3682 tasks      | elapsed: 61.6min\n",
      "[Parallel(n_jobs=8)]: Done 3769 tasks      | elapsed: 63.0min\n",
      "[Parallel(n_jobs=8)]: Done 3856 tasks      | elapsed: 64.3min\n",
      "[Parallel(n_jobs=8)]: Done 3945 tasks      | elapsed: 65.8min\n",
      "[Parallel(n_jobs=8)]: Done 4034 tasks      | elapsed: 67.2min\n",
      "[Parallel(n_jobs=8)]: Done 4125 tasks      | elapsed: 68.6min\n",
      "[Parallel(n_jobs=8)]: Done 4216 tasks      | elapsed: 69.9min\n",
      "[Parallel(n_jobs=8)]: Done 4309 tasks      | elapsed: 71.5min\n",
      "[Parallel(n_jobs=8)]: Done 4402 tasks      | elapsed: 72.8min\n",
      "[Parallel(n_jobs=8)]: Done 4497 tasks      | elapsed: 74.2min\n",
      "[Parallel(n_jobs=8)]: Done 4592 tasks      | elapsed: 75.6min\n",
      "[Parallel(n_jobs=8)]: Done 4689 tasks      | elapsed: 77.0min\n",
      "[Parallel(n_jobs=8)]: Done 4786 tasks      | elapsed: 78.5min\n",
      "[Parallel(n_jobs=8)]: Done 4885 tasks      | elapsed: 80.0min\n",
      "[Parallel(n_jobs=8)]: Done 4984 tasks      | elapsed: 81.4min\n",
      "[Parallel(n_jobs=8)]: Done 5085 tasks      | elapsed: 82.9min\n",
      "[Parallel(n_jobs=8)]: Done 5186 tasks      | elapsed: 84.5min\n",
      "[Parallel(n_jobs=8)]: Done 5289 tasks      | elapsed: 86.1min\n",
      "[Parallel(n_jobs=8)]: Done 5392 tasks      | elapsed: 87.7min\n",
      "[Parallel(n_jobs=8)]: Done 5497 tasks      | elapsed: 89.4min\n",
      "[Parallel(n_jobs=8)]: Done 5602 tasks      | elapsed: 91.0min\n",
      "[Parallel(n_jobs=8)]: Done 5709 tasks      | elapsed: 92.6min\n",
      "[Parallel(n_jobs=8)]: Done 5816 tasks      | elapsed: 94.2min\n",
      "[Parallel(n_jobs=8)]: Done 5925 tasks      | elapsed: 95.9min\n",
      "[Parallel(n_jobs=8)]: Done 6034 tasks      | elapsed: 97.5min\n",
      "[Parallel(n_jobs=8)]: Done 6145 tasks      | elapsed: 99.3min\n",
      "[Parallel(n_jobs=8)]: Done 6256 tasks      | elapsed: 101.0min\n",
      "[Parallel(n_jobs=8)]: Done 6369 tasks      | elapsed: 102.9min\n",
      "[Parallel(n_jobs=8)]: Done 6482 tasks      | elapsed: 104.5min\n",
      "[Parallel(n_jobs=8)]: Done 6597 tasks      | elapsed: 107.4min\n",
      "[Parallel(n_jobs=8)]: Done 6712 tasks      | elapsed: 110.4min\n",
      "[Parallel(n_jobs=8)]: Done 6829 tasks      | elapsed: 113.5min\n",
      "[Parallel(n_jobs=8)]: Done 6946 tasks      | elapsed: 116.1min\n",
      "[Parallel(n_jobs=8)]: Done 7065 tasks      | elapsed: 117.8min\n",
      "[Parallel(n_jobs=8)]: Done 7184 tasks      | elapsed: 119.6min\n",
      "[Parallel(n_jobs=8)]: Done 7305 tasks      | elapsed: 121.5min\n",
      "[Parallel(n_jobs=8)]: Done 7426 tasks      | elapsed: 123.2min\n",
      "[Parallel(n_jobs=8)]: Done 7549 tasks      | elapsed: 125.0min\n",
      "[Parallel(n_jobs=8)]: Done 7672 tasks      | elapsed: 127.0min\n",
      "[Parallel(n_jobs=8)]: Done 7797 tasks      | elapsed: 128.9min\n",
      "[Parallel(n_jobs=8)]: Done 7922 tasks      | elapsed: 130.7min\n",
      "[Parallel(n_jobs=8)]: Done 8049 tasks      | elapsed: 132.5min\n",
      "[Parallel(n_jobs=8)]: Done 8176 tasks      | elapsed: 134.4min\n",
      "[Parallel(n_jobs=8)]: Done 8305 tasks      | elapsed: 136.3min\n",
      "[Parallel(n_jobs=8)]: Done 8434 tasks      | elapsed: 138.2min\n",
      "[Parallel(n_jobs=8)]: Done 8565 tasks      | elapsed: 140.1min\n",
      "[Parallel(n_jobs=8)]: Done 8696 tasks      | elapsed: 142.0min\n",
      "[Parallel(n_jobs=8)]: Done 8829 tasks      | elapsed: 144.0min\n",
      "[Parallel(n_jobs=8)]: Done 8962 tasks      | elapsed: 146.0min\n",
      "[Parallel(n_jobs=8)]: Done 9080 out of 9080 | elapsed: 147.7min finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501 501 51174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=8)]: Done  45 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   45.9s\n",
      "[Parallel(n_jobs=8)]: Done  69 tasks      | elapsed:   56.6s\n",
      "[Parallel(n_jobs=8)]: Done  82 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=8)]: Done  97 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=8)]: Done 129 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=8)]: Done 165 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=8)]: Done 205 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=8)]: Done 226 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=8)]: Done 249 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=8)]: Done 297 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=8)]: Done 322 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=8)]: Done 376 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=8)]: Done 405 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=8)]: Done 465 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=8)]: Done 501 out of 501 | elapsed:  7.5min finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 499 50835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=8)]: Done  45 tasks      | elapsed:   44.5s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   53.1s\n",
      "[Parallel(n_jobs=8)]: Done  69 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=8)]: Done  82 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=8)]: Done  97 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=8)]: Done 129 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=8)]: Done 165 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=8)]: Done 205 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=8)]: Done 226 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=8)]: Done 249 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=8)]: Done 297 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=8)]: Done 322 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=8)]: Done 376 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=8)]: Done 405 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=8)]: Done 465 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=8)]: Done 499 out of 499 | elapsed:  7.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9080 9080 958365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:   27.6s\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   38.2s\n",
      "[Parallel(n_jobs=8)]: Done  45 tasks      | elapsed:   46.3s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=8)]: Done  69 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=8)]: Done  82 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=8)]: Done  97 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=8)]: Done 129 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=8)]: Done 165 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=8)]: Done 205 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=8)]: Done 226 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=8)]: Done 249 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=8)]: Done 297 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=8)]: Done 322 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=8)]: Done 376 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=8)]: Done 405 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=8)]: Done 465 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=8)]: Done 496 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=8)]: Done 529 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=8)]: Done 562 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=8)]: Done 597 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=8)]: Done 632 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=8)]: Done 669 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=8)]: Done 706 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=8)]: Done 745 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed: 15.3min\n",
      "[Parallel(n_jobs=8)]: Done 825 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=8)]: Done 866 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=8)]: Done 909 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=8)]: Done 952 tasks      | elapsed: 20.3min\n",
      "[Parallel(n_jobs=8)]: Done 997 tasks      | elapsed: 21.0min\n",
      "[Parallel(n_jobs=8)]: Done 1042 tasks      | elapsed: 21.7min\n",
      "[Parallel(n_jobs=8)]: Done 1089 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=8)]: Done 1136 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=8)]: Done 1185 tasks      | elapsed: 23.8min\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed: 24.6min\n",
      "[Parallel(n_jobs=8)]: Done 1285 tasks      | elapsed: 25.5min\n",
      "[Parallel(n_jobs=8)]: Done 1336 tasks      | elapsed: 26.3min\n",
      "[Parallel(n_jobs=8)]: Done 1389 tasks      | elapsed: 27.2min\n",
      "[Parallel(n_jobs=8)]: Done 1442 tasks      | elapsed: 28.0min\n",
      "[Parallel(n_jobs=8)]: Done 1497 tasks      | elapsed: 28.8min\n",
      "[Parallel(n_jobs=8)]: Done 1552 tasks      | elapsed: 29.6min\n",
      "[Parallel(n_jobs=8)]: Done 1609 tasks      | elapsed: 30.5min\n",
      "[Parallel(n_jobs=8)]: Done 1666 tasks      | elapsed: 31.3min\n",
      "[Parallel(n_jobs=8)]: Done 1725 tasks      | elapsed: 32.2min\n",
      "[Parallel(n_jobs=8)]: Done 1784 tasks      | elapsed: 33.0min\n",
      "[Parallel(n_jobs=8)]: Done 1845 tasks      | elapsed: 34.0min\n",
      "[Parallel(n_jobs=8)]: Done 1906 tasks      | elapsed: 34.8min\n",
      "[Parallel(n_jobs=8)]: Done 1969 tasks      | elapsed: 35.8min\n",
      "[Parallel(n_jobs=8)]: Done 2032 tasks      | elapsed: 36.8min\n",
      "[Parallel(n_jobs=8)]: Done 2097 tasks      | elapsed: 37.8min\n",
      "[Parallel(n_jobs=8)]: Done 2162 tasks      | elapsed: 38.9min\n",
      "[Parallel(n_jobs=8)]: Done 2229 tasks      | elapsed: 39.9min\n",
      "[Parallel(n_jobs=8)]: Done 2296 tasks      | elapsed: 40.9min\n",
      "[Parallel(n_jobs=8)]: Done 2365 tasks      | elapsed: 42.0min\n",
      "[Parallel(n_jobs=8)]: Done 2434 tasks      | elapsed: 43.0min\n",
      "[Parallel(n_jobs=8)]: Done 2505 tasks      | elapsed: 44.1min\n",
      "[Parallel(n_jobs=8)]: Done 2576 tasks      | elapsed: 45.1min\n",
      "[Parallel(n_jobs=8)]: Done 2649 tasks      | elapsed: 46.2min\n",
      "[Parallel(n_jobs=8)]: Done 2722 tasks      | elapsed: 47.2min\n",
      "[Parallel(n_jobs=8)]: Done 2797 tasks      | elapsed: 48.4min\n",
      "[Parallel(n_jobs=8)]: Done 2872 tasks      | elapsed: 49.5min\n",
      "[Parallel(n_jobs=8)]: Done 2949 tasks      | elapsed: 50.7min\n",
      "[Parallel(n_jobs=8)]: Done 3026 tasks      | elapsed: 51.8min\n",
      "[Parallel(n_jobs=8)]: Done 3105 tasks      | elapsed: 53.0min\n",
      "[Parallel(n_jobs=8)]: Done 3184 tasks      | elapsed: 54.2min\n",
      "[Parallel(n_jobs=8)]: Done 3265 tasks      | elapsed: 55.4min\n",
      "[Parallel(n_jobs=8)]: Done 3346 tasks      | elapsed: 56.7min\n",
      "[Parallel(n_jobs=8)]: Done 3429 tasks      | elapsed: 57.9min\n",
      "[Parallel(n_jobs=8)]: Done 3512 tasks      | elapsed: 59.2min\n",
      "[Parallel(n_jobs=8)]: Done 3597 tasks      | elapsed: 60.4min\n",
      "[Parallel(n_jobs=8)]: Done 3682 tasks      | elapsed: 61.7min\n",
      "[Parallel(n_jobs=8)]: Done 3769 tasks      | elapsed: 63.0min\n",
      "[Parallel(n_jobs=8)]: Done 3856 tasks      | elapsed: 64.4min\n",
      "[Parallel(n_jobs=8)]: Done 3945 tasks      | elapsed: 65.8min\n",
      "[Parallel(n_jobs=8)]: Done 4034 tasks      | elapsed: 67.3min\n",
      "[Parallel(n_jobs=8)]: Done 4125 tasks      | elapsed: 68.7min\n",
      "[Parallel(n_jobs=8)]: Done 4216 tasks      | elapsed: 70.0min\n",
      "[Parallel(n_jobs=8)]: Done 4309 tasks      | elapsed: 71.6min\n",
      "[Parallel(n_jobs=8)]: Done 4402 tasks      | elapsed: 73.0min\n",
      "[Parallel(n_jobs=8)]: Done 4497 tasks      | elapsed: 74.3min\n",
      "[Parallel(n_jobs=8)]: Done 4592 tasks      | elapsed: 75.8min\n",
      "[Parallel(n_jobs=8)]: Done 4689 tasks      | elapsed: 77.2min\n",
      "[Parallel(n_jobs=8)]: Done 4786 tasks      | elapsed: 78.7min\n",
      "[Parallel(n_jobs=8)]: Done 4885 tasks      | elapsed: 81.6min\n",
      "[Parallel(n_jobs=8)]: Done 4984 tasks      | elapsed: 84.4min\n",
      "[Parallel(n_jobs=8)]: Done 5085 tasks      | elapsed: 87.3min\n",
      "[Parallel(n_jobs=8)]: Done 5186 tasks      | elapsed: 89.7min\n",
      "[Parallel(n_jobs=8)]: Done 5289 tasks      | elapsed: 91.4min\n",
      "[Parallel(n_jobs=8)]: Done 5392 tasks      | elapsed: 92.9min\n",
      "[Parallel(n_jobs=8)]: Done 5497 tasks      | elapsed: 94.6min\n",
      "[Parallel(n_jobs=8)]: Done 5602 tasks      | elapsed: 96.2min\n",
      "[Parallel(n_jobs=8)]: Done 5709 tasks      | elapsed: 97.9min\n",
      "[Parallel(n_jobs=8)]: Done 5816 tasks      | elapsed: 99.5min\n",
      "[Parallel(n_jobs=8)]: Done 5925 tasks      | elapsed: 101.2min\n",
      "[Parallel(n_jobs=8)]: Done 6034 tasks      | elapsed: 102.8min\n",
      "[Parallel(n_jobs=8)]: Done 6145 tasks      | elapsed: 104.5min\n",
      "[Parallel(n_jobs=8)]: Done 6256 tasks      | elapsed: 106.1min\n",
      "[Parallel(n_jobs=8)]: Done 6369 tasks      | elapsed: 107.9min\n",
      "[Parallel(n_jobs=8)]: Done 6482 tasks      | elapsed: 109.6min\n",
      "[Parallel(n_jobs=8)]: Done 6597 tasks      | elapsed: 111.4min\n",
      "[Parallel(n_jobs=8)]: Done 6712 tasks      | elapsed: 113.0min\n",
      "[Parallel(n_jobs=8)]: Done 6829 tasks      | elapsed: 114.6min\n",
      "[Parallel(n_jobs=8)]: Done 6946 tasks      | elapsed: 116.4min\n",
      "[Parallel(n_jobs=8)]: Done 7065 tasks      | elapsed: 118.2min\n",
      "[Parallel(n_jobs=8)]: Done 7184 tasks      | elapsed: 120.0min\n",
      "[Parallel(n_jobs=8)]: Done 7305 tasks      | elapsed: 121.8min\n",
      "[Parallel(n_jobs=8)]: Done 7426 tasks      | elapsed: 123.5min\n",
      "[Parallel(n_jobs=8)]: Done 7549 tasks      | elapsed: 125.3min\n",
      "[Parallel(n_jobs=8)]: Done 7672 tasks      | elapsed: 127.3min\n",
      "[Parallel(n_jobs=8)]: Done 7797 tasks      | elapsed: 129.2min\n",
      "[Parallel(n_jobs=8)]: Done 7922 tasks      | elapsed: 131.0min\n",
      "[Parallel(n_jobs=8)]: Done 8049 tasks      | elapsed: 132.8min\n",
      "[Parallel(n_jobs=8)]: Done 8176 tasks      | elapsed: 135.2min\n",
      "[Parallel(n_jobs=8)]: Done 8305 tasks      | elapsed: 137.2min\n",
      "[Parallel(n_jobs=8)]: Done 8434 tasks      | elapsed: 139.0min\n",
      "[Parallel(n_jobs=8)]: Done 8565 tasks      | elapsed: 141.0min\n",
      "[Parallel(n_jobs=8)]: Done 8696 tasks      | elapsed: 143.0min\n",
      "[Parallel(n_jobs=8)]: Done 8829 tasks      | elapsed: 145.2min\n",
      "[Parallel(n_jobs=8)]: Done 8962 tasks      | elapsed: 147.2min\n",
      "[Parallel(n_jobs=8)]: Done 9080 out of 9080 | elapsed: 150.5min finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501 501 51174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   29.6s\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   55.5s\n",
      "[Parallel(n_jobs=8)]: Done  45 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=8)]: Done  69 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=8)]: Done  82 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=8)]: Done  97 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=8)]: Done 129 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=8)]: Done 165 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=8)]: Done 205 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=8)]: Done 226 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=8)]: Done 249 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=8)]: Done 297 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=8)]: Done 322 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=8)]: Done 376 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=8)]: Done 405 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=8)]: Done 465 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=8)]: Done 501 out of 501 | elapsed: 10.4min finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 499 50835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=8)]: Done  45 tasks      | elapsed:   44.4s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   53.0s\n",
      "[Parallel(n_jobs=8)]: Done  69 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=8)]: Done  82 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=8)]: Done  97 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=8)]: Done 129 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=8)]: Done 165 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=8)]: Done 205 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=8)]: Done 226 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=8)]: Done 249 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=8)]: Done 297 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=8)]: Done 322 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=8)]: Done 376 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=8)]: Done 405 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=8)]: Done 465 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=8)]: Done 499 out of 499 | elapsed:  7.3min finished\n"
     ]
    }
   ],
   "source": [
    "runThisCell = True\n",
    "if runThisCell:\n",
    "    numSims = [12000]\n",
    "    numChannels = 1\n",
    "    sVariables = ['u', 'v', 'T']\n",
    "    #batchSize = 100\n",
    "\n",
    "\n",
    "    for sVar in sVariables:\n",
    "        for n in numSims:\n",
    "            dictToOpen = pathSave + \"processedDicts/Mars_new/\" + sVar + \"/Dict_NewMars_preProcessed_\" + str(numSims) + \"sims_0.9trPercent.txt\"\n",
    "            with open(dictToOpen, \"rb\") as myFile:\n",
    "                indexerDict = load(myFile)\n",
    "            Dict_processed_data = {}\n",
    "            data = getDataConvAE(n, pathSave, numChannels, sVar, indexerDict)\n",
    "            #Dict_processed_data['x'] = indexerDict[\"x\"] \n",
    "            #Dict_processed_data['y'] = indexerDict[\"y\"] \n",
    "            #Dict_processed_data['pMax'] = data.pMax\n",
    "            #Dict_processed_data['pMin'] = data.pMin\n",
    "\n",
    "            #with open(pathSave + \"processedDicts/Mars_new/ConvAE/\" + sVar + \"1/Dict_processed_data_MarsNew_2D_ConvAE_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel.txt\", \"wb\") as f:\n",
    "            #    dump(Dict_processed_data, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:37.156767Z",
     "iopub.status.busy": "2021-03-13T00:37:37.146275Z",
     "iopub.status.idle": "2021-03-13T00:37:51.980483Z",
     "shell.execute_reply": "2021-03-13T00:37:51.979910Z"
    }
   },
   "outputs": [],
   "source": [
    "class getDataConvAE_3D:\n",
    "    def __init__(self,_numSims, _pathSave, numChannels, sVar, indexerDict):\n",
    "        self.pathSave = _pathSave\n",
    "        self.numSims = _numSims  \n",
    "        self.sVar = sVar\n",
    "        self.indexerDict = indexerDict\n",
    "        \n",
    "        for s in range(3):\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "                \n",
    "            pathSaveE = pathSave + \"processedDicts/Mars_new/ConvAE/\" + sVar + \"/\" + stringInt\n",
    "            if not os.path.exists(pathSaveE):\n",
    "                os.makedirs(pathSaveE)\n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "            \n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "            \n",
    "            simList = []\n",
    "            timeList = []\n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    if dictInd != -1:\n",
    "                        timeList.append(innertimeList)\n",
    "                        simList.append(dictInd)\n",
    "                    dictInd = val[0]\n",
    "                    innertimeList = [val[2]]\n",
    "                else:\n",
    "                    innertimeList.append(val[2])    \n",
    "                    \n",
    "            simList.append(dictInd)\n",
    "            timeList.append(innertimeList)\n",
    "            \n",
    "            indexerVec = []\n",
    "            tol = 1e-3\n",
    "            \n",
    "            numSims = len(simList)\n",
    "            maxTimeSteps = 45\n",
    "            spacedTimeVec = np.linspace(0,0.04913813148788927,maxTimeSteps)\n",
    "            \n",
    "            for ind, sim in enumerate(simList):\n",
    "                innerTimeList = []\n",
    "                for time in spacedTimeVec:\n",
    "                    whereVec = np.where((time>=np.asarray(timeList[ind])-tol) & (time<=np.asarray(timeList[ind])+tol))[0]\n",
    "                    if len(whereVec) > 0:\n",
    "                        indexerVec.append([sim,0,timeList[ind][whereVec[-1]]])\n",
    "                    else:\n",
    "                        indexerVec.append([sim,0,-1])\n",
    "            \n",
    "            print(numSims)\n",
    "            \n",
    "            self.x_data = np.zeros((numSims,5))\n",
    "            self.y_data = np.zeros((numSims,maxTimeSteps,302,394,1))  \n",
    "            \n",
    "            print(\"memory allocated\")\n",
    "\n",
    "            numStateVariables = numChannels #T, x, y\n",
    "            sizeStateVariable = self.indexerDict[\"x\"].shape[0] \n",
    "\n",
    "            numLayers = 302 #202\n",
    "            sizeEachLayer = int(sizeStateVariable/numLayers)\n",
    "            \n",
    "            totalSamples = len(indexerVec)\n",
    "            numBatches = len(simList) #int(np.ceil(totalSamples/self.batchSize)) \n",
    "                                        \n",
    "            dictPrev = -1\n",
    "            dictInd = -1            \n",
    "            for ind, i in enumerate(indexerVec):\n",
    "                if i[0] != dictPrev:\n",
    "                    dictToOpen = \"/plp_user/agar_si/gaia/python/Data/2D/Mars_new/\" + sVar + \"/savedDict_profiles_Mars_new_2D_\" \\\n",
    "                                                                            + sVar + \"_only_Dict\" + str(i[0]) + \".txt\"\n",
    "                    with open(dictToOpen, \"rb\") as myFile:\n",
    "                        profiles = load(myFile)\n",
    "                    \n",
    "                   \n",
    "                    dictPrev = i[0]\n",
    "                    timeStep = 0\n",
    "                    dictInd += 1\n",
    "                    \n",
    "                    self.x_data[dictInd,:] = [np.log10(float(profiles['InputValues'+ str(i[1])][1].split('=')[1])), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][5].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][6].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][7].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][8].split('=')[1].split(\"_\")[0].split(\".ini\")[0])]\n",
    "\n",
    "                if i[2] != -1:\n",
    "                    field = profiles['Dict_2D_' + sVar + '_time_' + str(i[2]) + '_' + str(i[1])]\n",
    "                    field.shape = (1,numLayers,sizeEachLayer,numStateVariables)\n",
    "\n",
    "                    #field = (field-self.oMin)/(self.oMax-self.oMin) \n",
    "                    self.y_data[dictInd,timeStep,:,:,:] = field #encoder.predict(field)\n",
    "                \n",
    "                timeStep += 1\n",
    "            \n",
    "            def normalizeMatrix(m):\n",
    "                paraMax = np.amax(m[:,:],axis=0)\n",
    "                paraMin = np.amin(m[:,:],axis=0)\n",
    "                if s==0:\n",
    "                    self.xMax = paraMax\n",
    "                    self.xMin = paraMin\n",
    "                else:\n",
    "                    paraMax = self.xMax\n",
    "                    paraMin = self.xMin    \n",
    "                paraNorms = paraMax-paraMin\n",
    "                for ind,val in enumerate(paraNorms):\n",
    "                    if val > 0:\n",
    "                        m[:,ind] = np.divide(m[:,ind]-paraMin[ind],val)\n",
    "                return m        \n",
    "            \n",
    "            self.x_data = normalizeMatrix(self.x_data)\n",
    "            \n",
    "            print(self.xMax, self.xMin) #, self.deltatMax, self.deltatMin)\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(self.y_data[0:10,:,:,:,:].flatten(),'b.')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()            \n",
    "            plt.plot(self.x_data[0:10,:].flatten(),'r.')\n",
    "            plt.show()\n",
    "            \n",
    "            indices = np.arange(self.x_data.shape[0])       \n",
    "            random.seed(6)\n",
    "            random.shuffle(indices)\n",
    "            #self.y_data = self.y_data[indices,:,:,:,:]\n",
    "            #self.x_data = self.x_data[indices,:]\n",
    "            \n",
    "            if s == 0:\n",
    "                print(\"Training data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/ConvAE/\" + sVar + \"/Dict_processed_data_MarsNew_3DConv_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_regIndexerVec_data.txt\", \"wb\") as f:\n",
    "                    dump(indexerVec , f, protocol=4)\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/ConvAE/\" + sVar + \"/Dict_processed_data_MarsNew_3DConv_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_data.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data] , f, protocol=4)\n",
    "            if s == 1:\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/ConvAE/\" + sVar + \"/Dict_processed_data_MarsNew_3DConv_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_regIndexerVec_cv.txt\", \"wb\") as f:\n",
    "                    dump(indexerVec , f, protocol=4)\n",
    "                print(\"CV data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/ConvAE/\" + sVar + \"/Dict_processed_data_MarsNew_3DConv_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_cv.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data] , f, protocol=4)\n",
    "            if s == 2:\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/ConvAE/\" + sVar + \"/Dict_processed_data_MarsNew_3DConv_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_regIndexerVec_test.txt\", \"wb\") as f:\n",
    "                    dump(indexerVec , f, protocol=4)\n",
    "                print(\"Test data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/ConvAE/\" + sVar + \"/Dict_processed_data_MarsNew_3DConv_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_test.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data] , f, protocol=4)\n",
    "        \n",
    "        profiles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:51.985870Z",
     "iopub.status.busy": "2021-03-13T00:37:51.985366Z",
     "iopub.status.idle": "2021-03-13T00:37:52.033303Z",
     "shell.execute_reply": "2021-03-13T00:37:52.033729Z"
    }
   },
   "outputs": [],
   "source": [
    "runThisCell = False\n",
    "if runThisCell:\n",
    "    numSims = [12000]\n",
    "    numChannels = 1\n",
    "    sVariables = ['T'] #, 'u', 'v'] #, 'T']\n",
    "    #batchSize = 100\n",
    "\n",
    "    for sVar in sVariables:\n",
    "        for n in numSims:\n",
    "            dictToOpen = pathSave + \"processedDicts/Mars_new/\" + sVar + \"/Dict_NewMars_preProcessed_\" + str(numSims) + \"sims_0.98trPercent.txt\"\n",
    "            with open(dictToOpen, \"rb\") as myFile:\n",
    "                indexerDict = load(myFile)\n",
    "            Dict_processed_data = {}\n",
    "            data = getDataConvAE_3D(n, pathSave, numChannels, sVar, indexerDict)\n",
    "            #Dict_processed_data['x'] = indexerDict[\"x\"] \n",
    "            #Dict_processed_data['y'] = indexerDict[\"y\"] \n",
    "            #Dict_processed_data['pMax'] = data.pMax\n",
    "            #Dict_processed_data['pMin'] = data.pMin\n",
    "\n",
    "            #with open(pathSave + \"processedDicts/Mars_new/ConvAE/\" + sVar + \"1/Dict_processed_data_MarsNew_2D_ConvAE_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel.txt\", \"wb\") as f:\n",
    "            #    dump(Dict_processed_data, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.058466Z",
     "iopub.status.busy": "2021-03-13T00:37:52.041456Z",
     "iopub.status.idle": "2021-03-13T00:37:52.105919Z",
     "shell.execute_reply": "2021-03-13T00:37:52.106343Z"
    }
   },
   "outputs": [],
   "source": [
    "class getDataNN_POD:\n",
    "    def __init__(self,_numSims, _pathSave, numChannels, sVar, indexerDict):\n",
    "        self.pathSave = _pathSave\n",
    "        self.numSims = _numSims  \n",
    "        self.sVar = sVar\n",
    "        self.indexerDict = indexerDict\n",
    "        \n",
    "        ae = ['T_Mars300_sims10k_f7A11s3_c369', \"conv2d_3\", 1620, [12,15,9], -7] \n",
    "        autoencoder = load_model(self.pathSave + '/TrainedNetworks/autoencoder/' + ae[0] + '.hdf5')\n",
    "        autoencoder.summary()\n",
    "        encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(ae[1]).output)\n",
    "        \n",
    "        dictToOpen = \"processedDicts/Mars_new/T/Dict_NewMars_preProcessed_\" + str([12000]) + \"sims_0.98trPercent.txt\"\n",
    "        with open(self.pathSave + dictToOpen, \"rb\") as myFile:\n",
    "            Dict_processed_data_monly = load(myFile)\n",
    "        \n",
    "        self.oMax = Dict_processed_data_monly['pMax']\n",
    "        self.oMin = Dict_processed_data_monly['pMin']\n",
    "        \n",
    "        def POD(X,r):\n",
    "            U,Sigma,VT = np.linalg.svd(X,full_matrices=0) # Step 1\n",
    "            Ur = U[:,:r]\n",
    "            Sigmar = np.diag(Sigma[:r])\n",
    "            VTr = VT[:r,:]\n",
    "            return Ur, Sigmar, VTr\n",
    "        \n",
    "        self.Ur_min = 1e+16\n",
    "        self.Sigmar_min = 1e+16        \n",
    "        self.VTr_min = 1e+16\n",
    "\n",
    "        self.Ur_max = -1e+16\n",
    "        self.Sigmar_max = -1e+16\n",
    "        self.VTr_max = -1e+16\n",
    "        \n",
    "        numModes = 100\n",
    "\n",
    "        for s in range(3):\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "                \n",
    "            pathSaveE = pathSave + \"processedDicts/Mars_new/NN/\" + sVar + \"/\" + stringInt\n",
    "            if not os.path.exists(pathSaveE):\n",
    "                os.makedirs(pathSaveE)\n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "            \n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "            \n",
    "            simList = []\n",
    "            timeList = []\n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    if dictInd != -1:\n",
    "                        timeList.append(innertimeList)\n",
    "                        simList.append(dictInd)\n",
    "                    dictInd = val[0]\n",
    "                    innertimeList = [val[2]]\n",
    "                else:\n",
    "                    innertimeList.append(val[2])    \n",
    "                    \n",
    "            simList.append(dictInd)\n",
    "            timeList.append(innertimeList)\n",
    "            \n",
    "            indexerVec = []\n",
    "            tol = 1e-3\n",
    "            \n",
    "            numSims = len(simList)\n",
    "            maxTimeSteps = numModes\n",
    "            spacedTimeVec = np.linspace(0,0.04913813148788927,maxTimeSteps)\n",
    "            spaced_timeList = []\n",
    "            \n",
    "            for ind, sim in enumerate(simList):\n",
    "                innerTimeList = []\n",
    "                for time in spacedTimeVec:\n",
    "                    whereVec = np.where((time>=np.asarray(timeList[ind])-tol) & (time<=np.asarray(timeList[ind])+tol))[0]\n",
    "                    if len(whereVec) > 0:\n",
    "                        innerTimeList.append(timeList[ind][whereVec[-1]])\n",
    "                    else:\n",
    "                        innerTimeList.append(-1)\n",
    "                spaced_timeList.append(innerTimeList)\n",
    "            \n",
    "            print(numSims)            \n",
    "            self.x_data = np.zeros((numSims*numModes,6))\n",
    "            self.y_data = np.zeros((numSims*numModes,ae[2]+1+numModes))  \n",
    "\n",
    "            numStateVariables = numChannels #T, x, y\n",
    "            sizeStateVariable = self.indexerDict[\"x\"].shape[0] \n",
    "\n",
    "            numLayers = 302 #202\n",
    "            sizeEachLayer = int(sizeStateVariable/numLayers)\n",
    "            \n",
    "            totalSamples = len(indexerVec)\n",
    "            numBatches = len(simList) #int(np.ceil(totalSamples/self.batchSize)) \n",
    "                                        \n",
    "            dictPrev = -1\n",
    "            dictInd = -1            \n",
    "            for ind, i in enumerate(simList):\n",
    "                if ind%10==0:\n",
    "                    print(ind)\n",
    "                dictToOpen = \"/plp_user/agar_si/gaia/python/Data/2D/Mars_new/\" + sVar + \"/savedDict_profiles_Mars_new_2D_\" \\\n",
    "                                                                            + sVar + \"_only_Dict\" + str(i) + \".txt\"\n",
    "                with open(dictToOpen, \"rb\") as myFile:\n",
    "                    profiles = load(myFile)\n",
    "                    \n",
    "                X = np.zeros((ae[2],maxTimeSteps))\n",
    "                for ind_t,t in enumerate(spaced_timeList[ind]):\n",
    "                    if t != -1:\n",
    "                        field = profiles['Dict_2D_' + sVar + '_time_' + str(t) + '_' + str(0)]\n",
    "                        field.shape = (1,numLayers,sizeEachLayer,numStateVariables)\n",
    "\n",
    "                        field = (field-self.oMin)/(self.oMax-self.oMin)                     \n",
    "                        X[:,ind_t] = encoder.predict(field).flatten()\n",
    "                \n",
    "                Ur, Sigmar, VTr = POD(X,numModes)\n",
    "                \n",
    "                for m in range(numModes):\n",
    "                    Sigmar[m,m] = np.log10(Sigmar[m,m])\n",
    "\n",
    "                if s==0:\n",
    "                    if np.amin(Ur) < self.Ur_min:\n",
    "                        self.Ur_min = np.amin(Ur)\n",
    "                    if np.amin(Sigmar) < self.Sigmar_min:\n",
    "                        self.Sigmar_min = np.amin(Sigmar)\n",
    "                    if np.amin(VTr) < self.VTr_min:\n",
    "                        self.VTr_min = np.amin(VTr)\n",
    "\n",
    "                    if np.amax(Ur) > self.Ur_max:\n",
    "                        self.Ur_max = np.amax(Ur)\n",
    "                    if np.amax(Sigmar) > self.Sigmar_max:\n",
    "                        self.Sigmar_max = np.amax(Sigmar)\n",
    "                    if np.amax(VTr) > self.VTr_max:\n",
    "                        self.VTr_max = np.amax(VTr)\n",
    "                \n",
    "                for m in range(numModes):\n",
    "                    self.y_data[(ind*numModes)+m,:ae[2]] = Ur[:,m]\n",
    "                    self.y_data[(ind*numModes)+m,ae[2]:ae[2]+1] = Sigmar[m,m]\n",
    "                    self.y_data[(ind*numModes)+m,ae[2]+1:] = VTr[m,:]\n",
    "                    \n",
    "                    if spaced_timeList[ind][m] == -1:\n",
    "                        mode = -1\n",
    "                    else:\n",
    "                        mode = m\n",
    "                        \n",
    "                    self.x_data[(ind*numModes)+m,:] = [mode,\\\n",
    "                       np.log10(float(profiles['InputValues'+ str(0)][1].split('=')[1])), \\\n",
    "                                float(profiles['InputValues'+ str(0)][5].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(0)][6].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(0)][7].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(0)][8].split('=')[1].split(\"_\")[0].split(\".ini\")[0])]\n",
    "                    \n",
    "            print(self.x_data.shape, self.y_data.shape)\n",
    "            \n",
    "            discard_indices = []\n",
    "            for ind in range(self.y_data.shape[0]):\n",
    "                mode = int(self.x_data[ind,0])\n",
    "                if mode != -1:\n",
    "                    self.y_data[ind,:ae[2]] = (self.y_data[ind,:ae[2]]-self.Ur_min)/(self.Ur_max-self.Ur_min)\n",
    "                    self.y_data[ind,ae[2]:ae[2]+1] = (self.y_data[ind,ae[2]:ae[2]+1]-self.Sigmar_min)/(self.Sigmar_max-self.Sigmar_min)\n",
    "                    self.y_data[ind,ae[2]+1:] = (self.y_data[ind,ae[2]+1:]-self.VTr_min)/(self.VTr_max-self.VTr_min)\n",
    "                else:\n",
    "                    discard_indices.append(ind)\n",
    "                    \n",
    "            self.y_data = np.delete(self.y_data, discard_indices, axis=0)\n",
    "            self.x_data = np.delete(self.x_data, discard_indices, axis=0)\n",
    "            \n",
    "            print(self.x_data.shape, self.y_data.shape)  \n",
    "\n",
    "            def normalizeMatrix(m):\n",
    "                paraMax = np.amax(m[:,:],axis=0)\n",
    "                paraMin = np.amin(m[:,:],axis=0)\n",
    "                if s==0:\n",
    "                    self.xMax = paraMax\n",
    "                    self.xMin = paraMin\n",
    "                else:\n",
    "                    paraMax = self.xMax\n",
    "                    paraMin = self.xMin    \n",
    "                paraNorms = paraMax-paraMin\n",
    "                for ind,val in enumerate(paraNorms):\n",
    "                    if val > 0:\n",
    "                        m[:,ind] = np.divide(m[:,ind]-paraMin[ind],val)\n",
    "                return m        \n",
    "            \n",
    "            self.x_data = normalizeMatrix(self.x_data) \n",
    "            \n",
    "            print(self.xMax, self.xMin) \n",
    "            \n",
    "            indices = np.arange(self.x_data.shape[0])       \n",
    "            random.seed(6)\n",
    "            random.shuffle(indices)\n",
    "            self.y_data = self.y_data[indices,:]\n",
    "            self.x_data = self.x_data[indices,:]\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(self.y_data[0:1000,:].flatten(),'b.')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()            \n",
    "            plt.plot(self.x_data[0:1000,:].flatten(),'r.')\n",
    "            plt.show()\n",
    "            \n",
    "            if s == 0:\n",
    "                print(\"Training data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/NN/\" + sVar + \"/Dict_processed_data_MarsNew_NNpod_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_regIndexerVec_data.txt\", \"wb\") as f:\n",
    "                    dump(indexerVec , f, protocol=4)\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/NN/\" + sVar + \"/Dict_processed_data_MarsNew_NNpod_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_data.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data] , f, protocol=4)\n",
    "            if s == 1:\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/NN/\" + sVar + \"/Dict_processed_data_MarsNew_NNpod_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_regIndexerVec_cv.txt\", \"wb\") as f:\n",
    "                    dump(indexerVec , f, protocol=4)\n",
    "                print(\"CV data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/NN/\" + sVar + \"/Dict_processed_data_MarsNew_NNpod_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_cv.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data] , f, protocol=4)\n",
    "            if s == 2:\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/NN/\" + sVar + \"/Dict_processed_data_MarsNew_NNpod_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_regIndexerVec_test.txt\", \"wb\") as f:\n",
    "                    dump(indexerVec , f, protocol=4)\n",
    "                print(\"Test data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/NN/\" + sVar + \"/Dict_processed_data_MarsNew_NNpod_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_test.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data, self.xMax, self.xMin, self.Ur_min, self.Sigmar_min, self.VTr_min, self.Ur_max, self.Sigmar_max, self.VTr_max] , f, protocol=4)\n",
    "        \n",
    "        profiles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.110329Z",
     "iopub.status.busy": "2021-03-13T00:37:52.109830Z",
     "iopub.status.idle": "2021-03-13T00:37:52.143865Z",
     "shell.execute_reply": "2021-03-13T00:37:52.143419Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runThisCell = False\n",
    "if runThisCell:\n",
    "    numSims = [12000]\n",
    "    numChannels = 1\n",
    "    sVariables = ['T'] #, 'u', 'v'] #, 'T']\n",
    "    #batchSize = 100\n",
    "\n",
    "    for sVar in sVariables:\n",
    "        for n in numSims:\n",
    "            dictToOpen = pathSave + \"processedDicts/Mars_new/\" + sVar + \"/Dict_NewMars_preProcessed_\" + str(numSims) + \"sims_0.98trPercent.txt\"\n",
    "            with open(dictToOpen, \"rb\") as myFile:\n",
    "                indexerDict = load(myFile)\n",
    "            Dict_processed_data = {}\n",
    "            data = getDataNN_POD(n, pathSave, numChannels, sVar, indexerDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.151400Z",
     "iopub.status.busy": "2021-03-13T00:37:52.145979Z",
     "iopub.status.idle": "2021-03-13T00:37:52.213178Z",
     "shell.execute_reply": "2021-03-13T00:37:52.213651Z"
    }
   },
   "outputs": [],
   "source": [
    "class getDataConvNN_wiMelt:\n",
    "    def __init__(self,_numSims, _pathSave, numChannels, sVar, indexerDict):\n",
    "        self.pathSave = _pathSave\n",
    "        self.numSims = _numSims  \n",
    "        self.sVar = sVar\n",
    "        self.indexerDict = indexerDict\n",
    "        \n",
    "        ae = ['T_Mars300_sims10k_f7A11s3_c369_tanh_l2reg_wlrsc', \"conv2d_3\", 1620, [12,15,9], -7] \n",
    "        autoencoder = load_model(self.pathSave + '/TrainedNetworks/autoencoder/' + ae[0] + '.hdf5')\n",
    "        autoencoder.summary()\n",
    "        encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(ae[1]).output)\n",
    "        \n",
    "       \n",
    "        dictToOpen = \"/plp_user/agar_si/gaia/python/Data/1D/Compare_melt/savedDict_profiles_Mars_new_1D_Dict\" + str(0) + \".txt\"\n",
    "        with open(dictToOpen, \"rb\") as myFile:\n",
    "            profiles = load(myFile)\n",
    "        \n",
    "        self.rProf = profiles['Dict_Rprof_4p5Gyr' + str(0)]\n",
    "        self.rProf = (self.rProf - self.rProf.min())/(self.rProf.max()-self.rProf.min())\n",
    "        \n",
    "        self.oMax = indexerDict['pMax']\n",
    "        self.oMin = indexerDict['pMin']\n",
    "        \n",
    "        totalTime = 0.04913813148788927\n",
    "        H_U_238  = 9.46 * 1e-5 \n",
    "        H_U_235 = 5.69 * 1e-4 \n",
    "        H_Th_232 = 2.64 * 1e-5 \n",
    "        H_K_40 = 2.92 * 1e-5 \n",
    "        half_life_U_238 = 4.47 * 1e9 \n",
    "        half_life_U_235 = 7.04 * 1e8 \n",
    "        half_life_Th_232 = 1.40 * 1e10 \n",
    "        half_life_K_40 = 1.25 * 1e9 \n",
    "        year_to_seconds = 365.25 * 24. * 3600.\n",
    "        D = 1700e+3\n",
    "        rho_c = 7000. \n",
    "        rho_m   = 3500. \n",
    "        c_p_c = 850. \n",
    "        c_p_m = 1200. \n",
    "        r_m = 1700e+3\n",
    "        r_c = 1700e+3\n",
    "        alpha_m = 2.5e-5\n",
    "        alpha_c = 4.0e-5 \n",
    "        \n",
    "        T_delta = 2000\n",
    "        f2 = 1./(c_p_m)\n",
    "        f0 = 3./(rho_m*c_p_m) * np.power(r_c,2)/(np.power(r_c+r_m,3)-np.power(r_c,3))\n",
    "        f1 = 3./(rho_m*c_p_m) * np.power(r_c+r_m,2)/(np.power(r_c+r_m,3)-np.power(r_c,3))\n",
    "        d_0 = 2.47\n",
    "        d_1 = 0.33e-9\n",
    "        d_2 = 0.48\n",
    "        k_ref = 4\n",
    "        \n",
    "        k_diffusive = k_ref/(rho_m*c_p_m)        #1e-6 \n",
    "        g = 3.7\n",
    "        lambda_U_238 = np.log(2.)/(half_life_U_238 * year_to_seconds / np.power(D,2.) * k_diffusive)\n",
    "        lambda_U_235 = np.log(2.)/(half_life_U_235 * year_to_seconds / np.power(D,2.) * k_diffusive)\n",
    "        lambda_Th_232 = np.log(2.)/(half_life_Th_232 * year_to_seconds / np.power(D,2.) * k_diffusive)\n",
    "        lambda_K_40 = np.log(2.)/(half_life_K_40 * year_to_seconds / np.power(D,2.) * k_diffusive)\n",
    "\n",
    "        r_c = 1700e+3\n",
    "        r_p = 3400e+3\n",
    "        D = 1700e+3 \n",
    "        rho_crust = 2900.\n",
    "        crustal_thickness = 64.3e+3\n",
    "        r_crust = r_p - crustal_thickness\n",
    "        \n",
    "        m_cr = (np.power(r_p,3.) - np.power(r_crust,3.)) * rho_crust * 4/3 * np.pi\n",
    "        m_m = (np.power(r_crust,3.) - np.power(r_c,3.)) * rho_m * 4/3 * np.pi\n",
    "\n",
    "        def getH0(_t, depleteBy):\n",
    "            C_U_0 = 16.0 * 1e-9 * depleteBy\n",
    "            C_Th_0_232 = 56.0 * 1e-9 * depleteBy\n",
    "            C_K_0 = 305.0 * 1e-6 * depleteBy\n",
    "            _t = ((_t/k_diffusive) * np.power(D,2.))/year_to_seconds\n",
    "            H_t_U_238 = 0.9928 * C_U_0 * H_U_238  * np.exp(_t * np.log(2.) / half_life_U_238) \n",
    "            H_t_U_235 = 0.0072 * C_U_0 * H_U_235 * np.exp(_t * np.log(2.) / half_life_U_235)\n",
    "            H_t_Th_232 = C_Th_0_232 * H_Th_232 * np.exp(_t * np.log(2.) / half_life_Th_232) \n",
    "            H_t_K_40 = 1.19 * 1e-4 * C_K_0 * H_K_40 * np.exp(_t * np.log(2.) / half_life_K_40)\n",
    "            return H_t_U_238 + H_t_U_235 + H_t_Th_232 + H_t_K_40  \n",
    "\n",
    "        Tcmb = 250+T_delta\n",
    "        \n",
    "        def getParaT(Lambda_cr,tprof,t_current,t_prev):\n",
    "            tprof.shape = (302,)\n",
    "            tprof = tprof*T_delta+250\n",
    "            depleteBy = m_m/(m_cr*Lambda_cr + m_m)\n",
    "            deltaTimeD = (t_current-t_prev)/k_diffusive*np.power(D,2.)\n",
    "            k_top = (d_0 + d_1*(rho_m*g*0))*np.power(300/(tprof[-1]),d_2)\n",
    "            k_bot = (d_0 + d_1*(rho_m*g*D))*np.power(300/(tprof[0]),d_2)\n",
    "            qbot = -((tprof[0]-tprof[1]))/((self.rProf[0]-self.rProf[1])*D) * k_bot \n",
    "            qtop = -((tprof[-2]-tprof[-1]))/((self.rProf[-2]-self.rProf[-1])*D) * k_top\n",
    "            T_integ  = np.mean(tprof) + ((f0*qbot - f1*qtop + f2*getH0(totalTime-t_current,depleteBy)) * deltaTimeD)\n",
    "            return T_integ\n",
    "        \n",
    "        for s in range(3):\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "\n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "            \n",
    "            simList = []\n",
    "            timeList = []\n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    if dictInd != -1:\n",
    "                        timeList.append(innertimeList)\n",
    "                        simList.append(dictInd)\n",
    "                    dictInd = val[0]\n",
    "                    innertimeList = []\n",
    "                else:\n",
    "                    innertimeList.append(val[2])\n",
    "            \n",
    "            numStateVariables = numChannels #T, x, y\n",
    "            sizeStateVariable = self.indexerDict[\"x\"].shape[0] \n",
    "\n",
    "            numLayers = 302 #202\n",
    "            sizeEachLayer = int(sizeStateVariable/numLayers)\n",
    "            \n",
    "            totalSamples = 0\n",
    "            for iS in range(len(simList)):\n",
    "                totalSamples += len(timeList[iS])-1\n",
    "            print(totalSamples)\n",
    "            self.y_data = np.zeros((totalSamples,2+ae[3][0]*ae[3][1]*ae[3][2]))\n",
    "            self.x_data = np.zeros((totalSamples,1+6))\n",
    "                    \n",
    "            dictPrev = -1\n",
    "            ind = 0\n",
    "            for indSim, valSim in enumerate(simList):  \n",
    "                dictToOpen = \"/plp_user/agar_si/gaia/python/Data/2D/Mars_new/\" + sVar + \"/savedDict_profiles_Mars_new_2D_\" \\\n",
    "                                                                            + sVar + \"_only_Dict\" + str(valSim) + \".txt\"\n",
    "                with open(dictToOpen, \"rb\") as myFile:\n",
    "                    profiles = load(myFile)\n",
    "\n",
    "                for indTime, valTime in enumerate(timeList[indSim][:-1]):  \n",
    "                    t_current = valTime\n",
    "                    t_next = timeList[indSim][indTime+1]\n",
    "                    \n",
    "                    field = profiles['Dict_2D_' + sVar + '_time_' + str(t_current) + '_' + str(0)]\n",
    "                    field.shape = (1,numLayers,sizeEachLayer,numStateVariables)\n",
    "                    tProf = np.mean(field,axis=2,keepdims=False)\n",
    "                    \n",
    "                    field_next = profiles['Dict_2D_' + sVar + '_time_' + str(t_next) + '_' + str(0)]\n",
    "                    field_next.shape = (1,numLayers,sizeEachLayer,numStateVariables)\n",
    "                    tProf_next = np.mean(field_next)\n",
    "                    T_next = tProf_next*2000+250\n",
    "                    \n",
    "                    Lambda_cr = float(profiles['InputValues'+ str(0)][7].split('=')[1])\n",
    "                    meltedTemp = getParaT(Lambda_cr,tProf,t_next,t_current) - T_next\n",
    "                    self.y_data[ind,0] = T_next\n",
    "                    self.y_data[ind,1] = meltedTemp\n",
    "                    \n",
    "                    field = (field-self.oMin)/(self.oMax-self.oMin) \n",
    "                    self.y_data[ind,2:] = encoder.predict(field).flatten()\n",
    "\n",
    "                    self.x_data[ind,:] = [float(t_next), float(t_current), \\\n",
    "                            np.log10(float(profiles['InputValues'+ str(0)][1].split('=')[1])), \\\n",
    "                                    float(profiles['InputValues'+ str(0)][5].split('=')[1]), \\\n",
    "                                    float(profiles['InputValues'+ str(0)][6].split('=')[1]), \\\n",
    "                                    float(profiles['InputValues'+ str(0)][7].split('=')[1]), \\\n",
    "                                    float(profiles['InputValues'+ str(0)][8].split('=')[1].split(\"_\")[0].split(\".ini\")[0])]\n",
    "                    ind += 1\n",
    "                    \n",
    "            def normalizeMatrix(m):\n",
    "                paraMax = np.amax(m[:,0:7],axis=0)\n",
    "                paraMin = np.amin(m[:,0:7],axis=0)\n",
    "                if s==0:\n",
    "                    self.xMax = paraMax\n",
    "                    self.xMin = paraMin\n",
    "                else:\n",
    "                    paraMax = self.xMax\n",
    "                    paraMin = self.xMin    \n",
    "                paraNorms = paraMax-paraMin\n",
    "                for ind,val in enumerate(paraNorms):\n",
    "                    if val > 0 and ind<7:\n",
    "                        m[:,ind] = np.divide(m[:,ind]-paraMin[ind],val)\n",
    "                return m\n",
    "            \n",
    "            self.x_data = normalizeMatrix(self.x_data)\n",
    "            \n",
    "            #if s==0:\n",
    "            #    self.yMax = [np.amax(self.y_data[:,0]), np.amax(self.y_data[:,1]), np.amax(self.y_data[:,2:])]\n",
    "            #    self.yMin = [np.amin(self.y_data[:,0]), np.amin(self.y_data[:,1]), np.amin(self.y_data[:,2:])]\n",
    "\n",
    "            #self.y_data[:,0] = (self.y_data[:,0]-self.yMin[0])/(self.yMax[0]-self.yMin[0]) \n",
    "            self.y_data[:,1] = (self.y_data[:,1]-250)/2000 \n",
    "            self.y_data[:,0] = (self.y_data[:,0]-250)/2000\n",
    "            \n",
    "            print(self.xMax, self.xMin) #, self.yMax, self.yMin)\n",
    "            \n",
    "            indices = np.arange(self.x_data.shape[0])       \n",
    "            random.seed(6)\n",
    "            random.shuffle(indices)\n",
    "            self.x_data = self.x_data[indices,:]\n",
    "            self.y_data = self.y_data[indices,:]\n",
    "            \n",
    "            if s == 0:\n",
    "                print(\"Training data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/NN/\" + sVar + \"/Dict_processed_data_MarsNew_2D_NN_wMelt_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae1620_x_data.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "            if s == 1:\n",
    "                print(\"CV data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/NN/\" + sVar + \"/Dict_processed_data_MarsNew_2D_NN_wMelt_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae1620_x_cv.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "            if s == 2:\n",
    "                print(\"Test data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/NN/\" + sVar + \"/Dict_processed_data_MarsNew_2D_NN_wMelt_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae1620_x_test.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "\n",
    "        self.x = self.indexerDict[\"x\"]\n",
    "        self.y = self.indexerDict[\"y\"]\n",
    "        \n",
    "        profiles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.218926Z",
     "iopub.status.busy": "2021-03-13T00:37:52.218350Z",
     "iopub.status.idle": "2021-03-13T00:37:52.252634Z",
     "shell.execute_reply": "2021-03-13T00:37:52.252053Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "runThisCell=False\n",
    "if runThisCell:\n",
    "    numSims = [10000]\n",
    "    numChannels = 1\n",
    "    sVariables = ['T'] #, 'T']\n",
    "\n",
    "    dictToOpen = pathSave + \"processedDicts/Mars_new/T/Dict_NewMars_preProcessed_\" + str(numSims) + \"sims_0.9trPercent.txt\"\n",
    "    with open(dictToOpen, \"rb\") as myFile:\n",
    "        indexerDict = load(myFile)\n",
    "\n",
    "    for sVar in sVariables:\n",
    "        for n in numSims:\n",
    "            Dict_processed_data = {}\n",
    "            data = getDataConvNN_wiMelt(n, pathSave, numChannels, sVar, indexerDict)\n",
    "            Dict_processed_data['x'] = data.x \n",
    "            Dict_processed_data['y'] = data.y \n",
    "            Dict_processed_data['xMax'] = data.xMax\n",
    "            Dict_processed_data['xMin'] = data.xMin\n",
    "            #Dict_processed_data['yMax'] = data.yMax\n",
    "            #Dict_processed_data['yMin'] = data.yMin\n",
    "            Dict_processed_data['oMax'] = data.oMax\n",
    "            Dict_processed_data['oMin'] = data.oMin\n",
    "            Dict_processed_data['rProf'] = data.rProf\n",
    "\n",
    "            with open(pathSave+ \"processedDicts/Mars_new/NN/T/Dict_processed_data_MarsNew_2D_NN_wMelt_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae1620.txt\", \"wb\") as f:\n",
    "                dump(Dict_processed_data, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.265276Z",
     "iopub.status.busy": "2021-03-13T00:37:52.259924Z",
     "iopub.status.idle": "2021-03-13T00:37:52.308641Z",
     "shell.execute_reply": "2021-03-13T00:37:52.308032Z"
    }
   },
   "outputs": [],
   "source": [
    "class getDataConvNN:\n",
    "    def __init__(self,_numSims, _pathSave, numChannels, sVar, indexerDict, ae):\n",
    "        self.pathSave = _pathSave\n",
    "        self.numSims = _numSims  \n",
    "        self.sVar = sVar\n",
    "        self.indexerDict = indexerDict\n",
    "        \n",
    "        dictToOpen = \"processedDicts/Mars_new/T/Dict_NewMars_preProcessed_\" + str([12000]) + \"sims_0.9trPercent.txt\"\n",
    "        with open(self.pathSave + dictToOpen, \"rb\") as myFile:\n",
    "            Dict_processed_data_monly = load(myFile)\n",
    "        \n",
    "        self.oMax = Dict_processed_data_monly['pMax']\n",
    "        self.oMin = Dict_processed_data_monly['pMin']\n",
    "        \n",
    "        autoencoder = load_model(self.pathSave + '/TrainedNetworks/autoencoder/' + ae[0] + '.hdf5')\n",
    "        autoencoder.summary()\n",
    "        encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(ae[1]).output)\n",
    "        \n",
    "        for s in range(3):\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "\n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "                    \n",
    "            simList = []\n",
    "            timeList = []\n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    if dictInd != -1:\n",
    "                        timeList.append(innertimeList)\n",
    "                        simList.append(dictInd)\n",
    "                    dictInd = val[0]\n",
    "                    innertimeList = [val[2]]\n",
    "                else:\n",
    "                    innertimeList.append(val[2])    \n",
    "                    \n",
    "            simList.append(dictInd)\n",
    "            timeList.append(innertimeList)\n",
    "            \n",
    "            numStateVariables = numChannels #T, x, y\n",
    "            sizeStateVariable = self.indexerDict[\"x\"].shape[0] \n",
    "\n",
    "            numLayers = 302 #202\n",
    "            sizeEachLayer = int(sizeStateVariable/numLayers)\n",
    "            \n",
    "            totalSamples = 0\n",
    "            for iS in range(len(simList)):\n",
    "                totalSamples += len(timeList[iS])\n",
    "            \n",
    "            self.y_data = np.zeros((totalSamples,ae[3][0],ae[3][1],ae[3][2]))\n",
    "            self.x_data = np.zeros((totalSamples,6))\n",
    "                    \n",
    "            ind = 0\n",
    "            for indSim, valSim in enumerate(simList):  \n",
    "                print(indSim)\n",
    "                dictToOpen = \"/plp_user/agar_si/gaia/python/Data/2D/Mars_new/\" + sVar + \"/savedDict_profiles_Mars_new_2D_\" \\\n",
    "                                                                            + sVar + \"_only_Dict\" + str(valSim) + \".txt\"\n",
    "                with open(dictToOpen, \"rb\") as myFile:\n",
    "                    profiles = load(myFile)\n",
    "\n",
    "                for indTime, valTime in enumerate(timeList[indSim]):  \n",
    "                    t_current = valTime\n",
    "                    \n",
    "                    field = profiles['Dict_2D_' + sVar + '_time_' + str(t_current) + '_' + str(0)]\n",
    "                    field.shape = (1,numLayers,sizeEachLayer,numStateVariables)\n",
    "                    \n",
    "                    field = (field-self.oMin)/(self.oMax-self.oMin) \n",
    "                    self.y_data[ind,:] = encoder.predict(field)\n",
    "\n",
    "                    self.x_data[ind,:] = [float(t_current), \\\n",
    "                            np.log10(float(profiles['InputValues'+ str(0)][1].split('=')[1])), \\\n",
    "                                    float(profiles['InputValues'+ str(0)][5].split('=')[1]), \\\n",
    "                                    float(profiles['InputValues'+ str(0)][6].split('=')[1]), \\\n",
    "                                    float(profiles['InputValues'+ str(0)][7].split('=')[1]), \\\n",
    "                                    float(profiles['InputValues'+ str(0)][8].split('=')[1].split(\"_\")[0].split(\".ini\")[0])]\n",
    "                    ind += 1\n",
    "                    \n",
    "            def normalizeMatrix(m):\n",
    "                paraMax = np.amax(m[:,0:6],axis=0)\n",
    "                paraMin = np.amin(m[:,0:6],axis=0)\n",
    "                if s==0:\n",
    "                    self.xMax = paraMax\n",
    "                    self.xMin = paraMin\n",
    "                else:\n",
    "                    paraMax = self.xMax\n",
    "                    paraMin = self.xMin    \n",
    "                paraNorms = paraMax-paraMin\n",
    "                for ind,val in enumerate(paraNorms):\n",
    "                    if val > 0 and ind<6:\n",
    "                        m[:,ind] = np.divide(m[:,ind]-paraMin[ind],val)\n",
    "                return m\n",
    "            \n",
    "            self.x_data = normalizeMatrix(self.x_data)\n",
    "            \n",
    "            if s==0:\n",
    "                self.yMax = np.amax(self.y_data)\n",
    "                self.yMin = np.amin(self.y_data)\n",
    "                \n",
    "            self.y_data = (self.y_data-self.yMin)/(self.yMax-self.yMin) \n",
    "            \n",
    "            print(self.xMax, self.xMin, self.yMax, self.yMin)\n",
    "            \n",
    "            indices = np.arange(self.x_data.shape[0])       \n",
    "            random.seed(6)\n",
    "            random.shuffle(indices)\n",
    "            self.x_data = self.x_data[indices,:]\n",
    "            self.y_data = self.y_data[indices,:,:,:]\n",
    "            \n",
    "            if s == 0:\n",
    "                print(\"Training data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/NN/\" + sVar + \"/Dict_processed_data_MarsNew_2D_NN_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae\" + str(ae[2]) + \"_x_data_0p98tr.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "            if s == 1:\n",
    "                print(\"CV data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/NN/\" + sVar + \"/Dict_processed_data_MarsNew_2D_NN_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae\" + str(ae[2]) + \"_x_cv_0p98tr.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "            if s == 2:\n",
    "                print(\"Test data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/NN/\" + sVar + \"/Dict_processed_data_MarsNew_2D_NN_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae\" + str(ae[2]) + \"_x_test_0p98tr.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "\n",
    "        self.x = self.indexerDict[\"x\"]\n",
    "        self.y = self.indexerDict[\"y\"]\n",
    "        \n",
    "        profiles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.314066Z",
     "iopub.status.busy": "2021-03-13T00:37:52.313501Z",
     "iopub.status.idle": "2021-03-13T00:37:52.347978Z",
     "shell.execute_reply": "2021-03-13T00:37:52.347495Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runThisCell=False\n",
    "if runThisCell:\n",
    "    numSims = [12000]\n",
    "    numChannels = 1\n",
    "    sVariables = ['T'] #, 'T']\n",
    "    \n",
    "    dictToOpen = pathSave + \"processedDicts/Mars_new/T/Dict_NewMars_preProcessed_[12000]sims_0.98trPercent.txt\"\n",
    "    with open(dictToOpen, \"rb\") as myFile:\n",
    "        indexerDict = load(myFile)\n",
    "        \n",
    "    aeList = [['T_Mars300_sims10k_f5A7s2_c3to24_tanh_l2reg_wlrsc', \"conv2d_6\", 840, [5,7,24], -11]]\n",
    "    \n",
    "    for ae in aeList:\n",
    "        for sVar in sVariables:\n",
    "            for n in numSims:\n",
    "                Dict_processed_data = {}\n",
    "                data = getDataConvNN(n, pathSave, numChannels, sVar, indexerDict, ae)\n",
    "                Dict_processed_data['x'] = data.x \n",
    "                Dict_processed_data['y'] = data.y \n",
    "                Dict_processed_data['xMax'] = data.xMax\n",
    "                Dict_processed_data['xMin'] = data.xMin\n",
    "                Dict_processed_data['yMax'] = data.yMax\n",
    "                Dict_processed_data['yMin'] = data.yMin\n",
    "\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/NN/T/Dict_processed_data_MarsNew_2D_NN_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae\" + str(ae[2]) + \"_0p98tr.txt\", \"wb\") as f:\n",
    "                    dump(Dict_processed_data, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.370924Z",
     "iopub.status.busy": "2021-03-13T00:37:52.360494Z",
     "iopub.status.idle": "2021-03-13T00:37:52.413475Z",
     "shell.execute_reply": "2021-03-13T00:37:52.412981Z"
    }
   },
   "outputs": [],
   "source": [
    "class getDataConvLSTM:\n",
    "    def __init__(self,_numSims, _pathSave, numChannels, sVar, indexerDict):\n",
    "        self.pathSave = _pathSave\n",
    "        self.numSims = _numSims  \n",
    "        self.sVar = sVar\n",
    "        self.indexerDict = indexerDict\n",
    "        \n",
    "        ae = ['T_Mars300_sims10k_f5A7s2_c3to24_tanh_l2reg_wlrsc', \"conv2d_6\", 840, [5,7,24], -11]   \n",
    "        autoencoder = load_model(self.pathSave + '/TrainedNetworks/autoencoder/' + ae[0] + '.hdf5')\n",
    "        autoencoder.summary()\n",
    "        encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(ae[1]).output)   \n",
    "        \n",
    "        dictToOpen = \"processedDicts/Mars_new/T/Dict_NewMars_preProcessed_\" + str([12000]) + \"sims_0.9trPercent.txt\"\n",
    "        with open(self.pathSave + dictToOpen, \"rb\") as myFile:\n",
    "            Dict_processed_data_monly = load(myFile)\n",
    "        \n",
    "        self.oMax = Dict_processed_data_monly['pMax']\n",
    "        self.oMin = Dict_processed_data_monly['pMin']\n",
    "        \n",
    "        \n",
    "        maxTimeSteps = 0\n",
    "        for s in range(3):\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "            \n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "\n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "            \n",
    "            numSims = 0\n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    numSims += 1\n",
    "                    dictInd = val[0]\n",
    "                    timeSteps = 1 \n",
    "                else:\n",
    "                    timeSteps += 1\n",
    "                    if timeSteps > maxTimeSteps:\n",
    "                        maxTimeSteps = timeSteps\n",
    "        \n",
    "        for s in range(1,3):\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "\n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "            \n",
    "            numSims = 0\n",
    "            \n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    numSims += 1\n",
    "                    dictInd = val[0]\n",
    "                    timeSteps = 1 \n",
    "                else:\n",
    "                    timeSteps += 1\n",
    "                \n",
    "            print(numSims,maxTimeSteps)\n",
    "            \n",
    "            numStateVariables = numChannels #T, x, y\n",
    "            sizeStateVariable = self.indexerDict[\"x\"].shape[0] \n",
    "\n",
    "            numLayers = 302 #202\n",
    "            sizeEachLayer = int(sizeStateVariable/numLayers)\n",
    "            \n",
    "            totalSamples = len(indexerVec)\n",
    "            \n",
    "            self.x_data = np.zeros((numSims,maxTimeSteps,ae[3][0],ae[3][1],ae[3][2]+1))\n",
    "                    \n",
    "            dictPrev = -1\n",
    "            dictInd = -1\n",
    "            for ind, i in enumerate(indexerVec):\n",
    "                \n",
    "                if i[0] != dictPrev:\n",
    "                    dictToOpen = \"/plp_user/agar_si/gaia/python/Data/2D/Mars_new/\" + sVar + \"/savedDict_profiles_Mars_new_2D_\" \\\n",
    "                                                                            + sVar + \"_only_Dict\" + str(i[0]) + \".txt\"\n",
    "                    with open(dictToOpen, \"rb\") as myFile:\n",
    "                        profiles = load(myFile)\n",
    "                    dictPrev = i[0]\n",
    "                    timeStep = 0\n",
    "                    dictInd += 1\n",
    "\n",
    "                    print(dictInd,i)\n",
    "                \n",
    "                field = profiles['Dict_2D_' + sVar + '_time_' + str(i[2]) + '_' + str(i[1])]\n",
    "                field.shape = (1,numLayers,sizeEachLayer,numStateVariables)\n",
    "                \n",
    "                field = (field-self.oMin)/(self.oMax-self.oMin) \n",
    "                self.x_data[dictInd,timeStep,:,:,1:] = encoder.predict(field)\n",
    "                \n",
    "                if ind != len(indexerVec)-1 and indexerVec[ind][0] == indexerVec[ind+1][0]:\n",
    "                    t_delta = float(indexerVec[ind+1][2])-float(i[2])\n",
    "                    mask = 1\n",
    "                else:\n",
    "                    t_delta = 0\n",
    "                    mask = 0\n",
    "                    self.x_data[dictInd,timeStep+1:,0,0:7,0] = [mask, t_delta, \\\n",
    "                        np.log10(float(profiles['InputValues'+ str(i[1])][1].split('=')[1])), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][5].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][6].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][7].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][8].split('=')[1].split(\"_\")[0].split(\".ini\")[0])]\n",
    "\n",
    "                self.x_data[dictInd,timeStep,1,0,0] = float(i[2])\n",
    "                self.x_data[dictInd,timeStep,0,0:7,0] = [mask, t_delta, \\\n",
    "                        np.log10(float(profiles['InputValues'+ str(i[1])][1].split('=')[1])), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][5].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][6].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][7].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][8].split('=')[1].split(\"_\")[0].split(\".ini\")[0])]\n",
    "                timeStep += 1\n",
    "            \n",
    "            def normalizeMatrix(m):\n",
    "                paraMax = np.amax(m[:,:,0,0:7,0],axis=(0,1)).tolist()\n",
    "                paraMin = np.amin(m[:,:,0,0:7,0],axis=(0,1)).tolist()\n",
    "                \n",
    "                paraMax.insert(0,np.amax(self.x_data[:,:,1,0,0]))\n",
    "                paraMin.insert(0,np.amin(self.x_data[:,:,1,0,0]))\n",
    "                if s==0:\n",
    "                    self.xMax = paraMax\n",
    "                    self.xMin = paraMin\n",
    "                else:\n",
    "                    paraMax = self.xMax\n",
    "                    paraMin = self.xMin    \n",
    "                paraNorms = np.asarray(paraMax)-np.asarray(paraMin)\n",
    "                for ind,val in enumerate(paraNorms[1:]):\n",
    "                    if val > 0:\n",
    "                        m[:,:,0,ind,0] = np.divide(m[:,:,0,ind,0]-paraMin[ind+1],val)\n",
    "                        \n",
    "                m[:,:,1,0,0] = (m[:,:,1,0,0]-paraMin[0])/paraNorms[0]\n",
    "                return m        \n",
    "            \n",
    "            self.x_data = normalizeMatrix(self.x_data)\n",
    "            \n",
    "            if s==0:\n",
    "                self.yMax = np.amax(self.x_data[:,:,:,:,1:])\n",
    "                self.yMin = np.amin(self.x_data[:,:,:,:,1:])\n",
    "            \n",
    "            self.x_data[:,:,:,:,1:] = (self.x_data[:,:,:,:,1:]-self.yMin)/(self.yMax-self.yMin) \n",
    "            \n",
    "            print(self.xMax, self.xMin, self.yMax, self.yMin)\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(self.x_data[0:100,:,0,:,0].flatten(),'b.')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(self.x_data[0:100,:,1,0,0].flatten(),'g.')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(self.x_data[0:100,:,:,:,1:].flatten(),'r.')\n",
    "            plt.show()\n",
    "            \n",
    "            if s<2:\n",
    "                indices = np.arange(self.x_data.shape[0])       \n",
    "                random.seed(6)\n",
    "                random.shuffle(indices)\n",
    "                self.x_data = self.x_data[indices,:,:,:,:]\n",
    "            \n",
    "            if s == 0:\n",
    "                print(\"Training data: \" + str(self.x_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/ConvLSTM/\" + sVar + \"/Dict_processed_data_MarsNew_2D_ConvLSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae840_x_data_0p98tr_wt.txt\", \"wb\") as f:\n",
    "                    dump(self.x_data , f, protocol=4)\n",
    "            if s == 1:\n",
    "                print(\"CV data: \" + str(self.x_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/ConvLSTM/\" + sVar + \"/Dict_processed_data_MarsNew_2D_ConvLSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae840_x_cv_0p98tr_wt.txt\", \"wb\") as f:\n",
    "                    dump(self.x_data, f, protocol=4)\n",
    "            if s == 2:\n",
    "                print(\"Test data: \" + str(self.x_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/ConvLSTM/\" + sVar + \"/Dict_processed_data_MarsNew_2D_ConvLSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae840_x_test_0p98tr_wt.txt\", \"wb\") as f:\n",
    "                    dump(self.x_data, f, protocol=4)\n",
    "\n",
    "        self.x = self.indexerDict[\"x\"]\n",
    "        self.y = self.indexerDict[\"y\"]\n",
    "        \n",
    "        profiles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.418720Z",
     "iopub.status.busy": "2021-03-13T00:37:52.418126Z",
     "iopub.status.idle": "2021-03-13T00:37:52.451374Z",
     "shell.execute_reply": "2021-03-13T00:37:52.450889Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runThisCell=False\n",
    "if runThisCell:\n",
    "    numSims = [12000]\n",
    "    numChannels = 1\n",
    "    sVariables = ['T'] #, 'T']\n",
    "\n",
    "    dictToOpen = pathSave + \"processedDicts/Mars_new/T/Dict_NewMars_preProcessed_\" + str(numSims) + \"sims_0.98trPercent.txt\"\n",
    "    with open(dictToOpen, \"rb\") as myFile:\n",
    "        indexerDict = load(myFile)\n",
    "\n",
    "    for sVar in sVariables:\n",
    "        for n in numSims:\n",
    "            Dict_processed_data = {}\n",
    "            data = getDataConvLSTM(n, pathSave, numChannels, sVar, indexerDict)\n",
    "            Dict_processed_data['x'] = data.x \n",
    "            Dict_processed_data['y'] = data.y \n",
    "            Dict_processed_data['paraMax'] = data.xMax\n",
    "            Dict_processed_data['paraMin'] = data.xMin\n",
    "            Dict_processed_data['postEncodedOMax'] = data.yMax\n",
    "            Dict_processed_data['postEncodedOMin'] = data.yMin\n",
    "\n",
    "            with open(pathSave+ \"processedDicts/Mars_new/ConvLSTM/T/Dict_processed_data_MarsNew_2D_ConvLSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae840_0p98tr_wt.txt\", \"wb\") as f:\n",
    "                dump(Dict_processed_data, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.469183Z",
     "iopub.status.busy": "2021-03-13T00:37:52.463969Z",
     "iopub.status.idle": "2021-03-13T00:37:52.516116Z",
     "shell.execute_reply": "2021-03-13T00:37:52.515604Z"
    }
   },
   "outputs": [],
   "source": [
    "class getDataLSTM:\n",
    "    def __init__(self,_numSims, _pathSave, numChannels, sVar, indexerDict):\n",
    "        self.pathSave = _pathSave\n",
    "        self.numSims = _numSims  \n",
    "        self.sVar = sVar\n",
    "        self.indexerDict = indexerDict\n",
    "        \n",
    "        ae = ['T_Mars300_sims10k_f5A7s2_c3to24_tanh_l2reg_wlrsc', \"conv2d_6\", 840, [5,7,24], -11] \n",
    "        #ae = ['T_Mars300_sims10k_f7A11s3_c369', \"conv2d_3\", 1620, [12,15,9], -7] \n",
    "        autoencoder = load_model(self.pathSave + '/TrainedNetworks/autoencoder/' + ae[0] + '.hdf5')\n",
    "        autoencoder.summary()\n",
    "        encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(ae[1]).output)   \n",
    "        \n",
    "        dictToOpen = \"processedDicts/Mars_new/T/Dict_NewMars_preProcessed_\" + str([12000]) + \"sims_0.9trPercent.txt\"\n",
    "        with open(self.pathSave + dictToOpen, \"rb\") as myFile:\n",
    "            Dict_processed_data_monly = load(myFile)\n",
    "        \n",
    "        self.oMax = Dict_processed_data_monly['pMax']\n",
    "        self.oMin = Dict_processed_data_monly['pMin']\n",
    "        \n",
    "        \n",
    "        maxTimeSteps = 0\n",
    "        for s in range(3):\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "            \n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "\n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "            \n",
    "            numSims = 0\n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    numSims += 1\n",
    "                    dictInd = val[0]\n",
    "                    timeSteps = 1 \n",
    "                else:\n",
    "                    timeSteps += 1\n",
    "                    if timeSteps > maxTimeSteps:\n",
    "                        maxTimeSteps = timeSteps\n",
    "        \n",
    "        for s in range(3):\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "\n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "            \n",
    "            numSims = 0\n",
    "            \n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    numSims += 1\n",
    "                    dictInd = val[0]\n",
    "                    timeSteps = 1 \n",
    "                else:\n",
    "                    timeSteps += 1\n",
    "                \n",
    "            print(numSims,maxTimeSteps)\n",
    "            \n",
    "            numStateVariables = numChannels #T, x, y\n",
    "            sizeStateVariable = self.indexerDict[\"x\"].shape[0] \n",
    "\n",
    "            numLayers = 302 #202\n",
    "            sizeEachLayer = int(sizeStateVariable/numLayers)\n",
    "            \n",
    "            totalSamples = len(indexerVec)\n",
    "            \n",
    "            self.y_data = np.zeros((numSims,maxTimeSteps,ae[3][0]*ae[3][1]*ae[3][2]))\n",
    "            self.x_data = np.zeros((numSims,maxTimeSteps,ae[3][0]*ae[3][1]*ae[3][2]+8))\n",
    "                    \n",
    "            dictPrev = -1\n",
    "            dictInd = -1\n",
    "            for ind, i in enumerate(indexerVec):\n",
    "                if ind%1000==0:\n",
    "                    print(str(ind/len(indexerVec)*100))\n",
    "                if i[0] != dictPrev:\n",
    "                    dictToOpen = \"/plp_user/agar_si/gaia/python/Data/2D/Mars_new/\" + sVar + \"/savedDict_profiles_Mars_new_2D_\" \\\n",
    "                                                                            + sVar + \"_only_Dict\" + str(i[0]) + \".txt\"\n",
    "                    with open(dictToOpen, \"rb\") as myFile:\n",
    "                        profiles = load(myFile)\n",
    "                    dictPrev = i[0]\n",
    "                    timeStep = 0\n",
    "                    dictInd += 1\n",
    "                    \n",
    "                field = profiles['Dict_2D_' + sVar + '_time_' + str(i[2]) + '_' + str(i[1])]\n",
    "                field.shape = (1,numLayers,sizeEachLayer,numStateVariables)\n",
    "\n",
    "                field = (field-self.oMin)/(self.oMax-self.oMin) \n",
    "                self.x_data[dictInd,timeStep,8:] = encoder.predict(field).flatten()\n",
    "                \n",
    "                if ind != len(indexerVec)-1 and indexerVec[ind][0] == indexerVec[ind+1][0]:\n",
    "                    t_delta = float(indexerVec[ind+1][2])-float(i[2])\n",
    "                    t_ = float(i[2])\n",
    "                    field = profiles['Dict_2D_' + sVar + '_time_' + str(indexerVec[ind+1][2]) + '_' + str(indexerVec[ind+1][1])]\n",
    "                    field.shape = (1,numLayers,sizeEachLayer,numStateVariables)\n",
    "\n",
    "                    field = (field-self.oMin)/(self.oMax-self.oMin) \n",
    "                    self.y_data[dictInd,timeStep,:] = encoder.predict(field).flatten()\n",
    "                    mask = 1\n",
    "                else:\n",
    "                    t_delta = 0\n",
    "                    mask = 0\n",
    "                    t_ = 0\n",
    "                    self.x_data[dictInd,timeStep+1:,0:8] = [mask, t_delta, t_,\\\n",
    "                        np.log10(float(profiles['InputValues'+ str(i[1])][1].split('=')[1])), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][5].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][6].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][7].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][8].split('=')[1].split(\"_\")[0].split(\".ini\")[0])]\n",
    "\n",
    "                self.x_data[dictInd,timeStep,0:8] = [mask, t_delta, t_, \\\n",
    "                        np.log10(float(profiles['InputValues'+ str(i[1])][1].split('=')[1])), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][5].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][6].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][7].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][8].split('=')[1].split(\"_\")[0].split(\".ini\")[0])]\n",
    "                timeStep += 1\n",
    "            \n",
    "            def normalizeMatrix(m):\n",
    "                paraMax = np.amax(m[:,:,0:8],axis=(0,1))\n",
    "                paraMin = np.amin(m[:,:,0:8],axis=(0,1))\n",
    "                if s==0:\n",
    "                    self.xMax = paraMax\n",
    "                    self.xMin = paraMin\n",
    "                else:\n",
    "                    paraMax = self.xMax\n",
    "                    paraMin = self.xMin    \n",
    "                paraNorms = paraMax-paraMin\n",
    "                for ind,val in enumerate(paraNorms):\n",
    "                    if val > 0:\n",
    "                        m[:,:,ind] = np.divide(m[:,:,ind]-paraMin[ind],val)\n",
    "                return m        \n",
    "            \n",
    "            self.x_data = normalizeMatrix(self.x_data)\n",
    "            \n",
    "            if s==0:\n",
    "                self.yMax = np.amax(self.y_data)\n",
    "                self.yMin = np.amin(self.y_data)\n",
    "\n",
    "            #print(\"NOT normalizing encoded representation between 0 and 1\")\n",
    "            self.y_data = (self.y_data-self.yMin)/(self.yMax-self.yMin) \n",
    "            self.x_data[:,:,8:] = (self.x_data[:,:,8:]-self.yMin)/(self.yMax-self.yMin) \n",
    "            \n",
    "            print(self.xMax, self.xMin, self.yMax, self.yMin)\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(self.x_data[0:100,:,:].flatten(),'b.')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()            \n",
    "            plt.plot(self.y_data[0:100,:,:].flatten(),'r.')\n",
    "            plt.show()\n",
    "            \n",
    "            indices = np.arange(self.x_data.shape[0])       \n",
    "            random.seed(6)\n",
    "            random.shuffle(indices)\n",
    "            self.x_data = self.x_data[indices,:,:]\n",
    "            self.y_data = self.y_data[indices,:,:]\n",
    "            \n",
    "            if s == 0:\n",
    "                print(\"Training data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/LSTM/\" + sVar + \"/Dict_processed_data_MarsNew_2D_LSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_data_ae840.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data] , f, protocol=4)\n",
    "            if s == 1:\n",
    "                print(\"CV data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/LSTM/\" + sVar + \"/Dict_processed_data_MarsNew_2D_LSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_cv_ae840.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "            if s == 2:\n",
    "                print(\"Test data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/LSTM/\" + sVar + \"/Dict_processed_data_MarsNew_2D_LSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_test_ae840.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "\n",
    "        self.x = self.indexerDict[\"x\"]\n",
    "        self.y = self.indexerDict[\"y\"]\n",
    "        \n",
    "        profiles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.521292Z",
     "iopub.status.busy": "2021-03-13T00:37:52.520709Z",
     "iopub.status.idle": "2021-03-13T00:37:52.554417Z",
     "shell.execute_reply": "2021-03-13T00:37:52.554879Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runThisCell=False\n",
    "if runThisCell:\n",
    "    numSims = [12000]\n",
    "    numChannels = 1\n",
    "    sVariables = ['T'] #, 'T']\n",
    "\n",
    "    dictToOpen = pathSave + \"processedDicts/Mars_new/T/Dict_NewMars_preProcessed_\" + str(numSims) + \"sims_0.98trPercent.txt\"\n",
    "    with open(dictToOpen, \"rb\") as myFile:\n",
    "        indexerDict = load(myFile)\n",
    "\n",
    "    for sVar in sVariables:\n",
    "        for n in numSims:\n",
    "            Dict_processed_data = {}\n",
    "            data = getDataLSTM(n, pathSave, numChannels, sVar, indexerDict)\n",
    "            Dict_processed_data['x'] = data.x \n",
    "            Dict_processed_data['y'] = data.y \n",
    "            Dict_processed_data['paraMax'] = data.xMax\n",
    "            Dict_processed_data['paraMin'] = data.xMin\n",
    "            Dict_processed_data['postEncodedOMax'] = data.yMax\n",
    "            Dict_processed_data['postEncodedOMin'] = data.yMin\n",
    "\n",
    "            with open(pathSave+ \"processedDicts/Mars_new/LSTM/T/Dict_processed_data_MarsNew_2D_LSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae840.txt\", \"wb\") as f:\n",
    "                dump(Dict_processed_data, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.577783Z",
     "iopub.status.busy": "2021-03-13T00:37:52.567594Z",
     "iopub.status.idle": "2021-03-13T00:37:52.615317Z",
     "shell.execute_reply": "2021-03-13T00:37:52.614734Z"
    }
   },
   "outputs": [],
   "source": [
    "class getDataLSTM_uniform:\n",
    "    def __init__(self,_numSims, _pathSave, numChannels, sVar, indexerDict):\n",
    "        self.pathSave = _pathSave\n",
    "        self.numSims = _numSims  \n",
    "        self.sVar = sVar\n",
    "        self.indexerDict = indexerDict\n",
    "        \n",
    "        ae = ['T_Mars300_sims10k_f5A7s2_c3to24_tanh_l2reg_wlrsc', \"conv2d_6\", 840, [5,7,24], -11] \n",
    "        #ae = ['T_Mars300_sims10k_f7A11s3_c369', \"conv2d_3\", 1620, [12,15,9], -7] \n",
    "        autoencoder = load_model(self.pathSave + '/TrainedNetworks/autoencoder/' + ae[0] + '.hdf5')\n",
    "        autoencoder.summary()\n",
    "        encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(ae[1]).output)   \n",
    "        \n",
    "        dictToOpen = \"processedDicts/Mars_new/T/Dict_NewMars_preProcessed_\" + str([12000]) + \"sims_0.9trPercent.txt\"\n",
    "        with open(self.pathSave + dictToOpen, \"rb\") as myFile:\n",
    "            Dict_processed_data_monly = load(myFile)\n",
    "        \n",
    "        self.oMax = Dict_processed_data_monly['pMax']\n",
    "        self.oMin = Dict_processed_data_monly['pMin']            \n",
    "            \n",
    "        for s in range(3):\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "\n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "            \n",
    "            simList = []\n",
    "            timeList = []\n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    if dictInd != -1:\n",
    "                        timeList.append(innertimeList)\n",
    "                        simList.append(dictInd)\n",
    "                    dictInd = val[0]\n",
    "                    innertimeList = [val[2]]\n",
    "                else:\n",
    "                    innertimeList.append(val[2])    \n",
    "                    \n",
    "            simList.append(dictInd)\n",
    "            timeList.append(innertimeList)\n",
    "            \n",
    "            indexerVec = []\n",
    "            tol = 1e-3\n",
    "            \n",
    "            numSims = len(simList)\n",
    "            maxTimeSteps = 100\n",
    "            spacedTimeVec = np.linspace(0,0.04913813148788927,maxTimeSteps)\n",
    "            spaced_timeList = []\n",
    "            \n",
    "            for ind, sim in enumerate(simList):\n",
    "                innerTimeList = []\n",
    "                for time in spacedTimeVec:\n",
    "                    whereVec = np.where((time>=np.asarray(timeList[ind])-tol) & (time<=np.asarray(timeList[ind])+tol))[0]\n",
    "                    if len(whereVec) > 0:\n",
    "                        innerTimeList.append(timeList[ind][whereVec[-1]])\n",
    "                    else:\n",
    "                        innerTimeList.append(-1)\n",
    "                spaced_timeList.append(innerTimeList)\n",
    "            \n",
    "            print(numSims)            \n",
    "            self.x_data = np.zeros((numSims,maxTimeSteps,ae[3][0]*ae[3][1]*ae[3][2]+6))\n",
    "            self.y_data = np.zeros((numSims,maxTimeSteps,ae[3][0]*ae[3][1]*ae[3][2]))  \n",
    "\n",
    "            numStateVariables = numChannels #T, x, y\n",
    "            sizeStateVariable = self.indexerDict[\"x\"].shape[0] \n",
    "\n",
    "            numLayers = 302 #202\n",
    "            sizeEachLayer = int(sizeStateVariable/numLayers)\n",
    "            \n",
    "            for ind, i in enumerate(simList):\n",
    "                if ind%10==0:\n",
    "                    print(ind)\n",
    "                dictToOpen = \"/plp_user/agar_si/gaia/python/Data/2D/Mars_new/\" + sVar + \"/savedDict_profiles_Mars_new_2D_\" \\\n",
    "                                                                            + sVar + \"_only_Dict\" + str(i) + \".txt\"\n",
    "                with open(dictToOpen, \"rb\") as myFile:\n",
    "                    profiles = load(myFile)\n",
    "                    \n",
    "                for ind_t,t in enumerate(spaced_timeList[ind]):\n",
    "                    if t != -1:\n",
    "                        field = profiles['Dict_2D_' + sVar + '_time_' + str(t) + '_' + str(0)]\n",
    "                        field.shape = (1,numLayers,sizeEachLayer,numStateVariables)\n",
    "\n",
    "                        field = (field-self.oMin)/(self.oMax-self.oMin)      \n",
    "                        field = encoder.predict(field).flatten()\n",
    "                        self.x_data[ind,ind_t,6:] = field\n",
    "                        if ind_t <99:\n",
    "                            self.y_data[ind,ind_t+1,:] = field\n",
    "                        mask = 1\n",
    "                    else:\n",
    "                        mask = 0\n",
    "                    \n",
    "                    self.x_data[ind,ind_t,0:6] = [mask, \\\n",
    "                            np.log10(float(profiles['InputValues'+ str(0)][1].split('=')[1])), \\\n",
    "                                    float(profiles['InputValues'+ str(0)][5].split('=')[1]), \\\n",
    "                                    float(profiles['InputValues'+ str(0)][6].split('=')[1]), \\\n",
    "                                    float(profiles['InputValues'+ str(0)][7].split('=')[1]), \\\n",
    "                                    float(profiles['InputValues'+ str(0)][8].split('=')[1].split(\"_\")[0].split(\".ini\")[0])] \n",
    "            \n",
    "            def normalizeMatrix(m):\n",
    "                paraMax = np.amax(m[:,:,0:6],axis=(0,1))\n",
    "                paraMin = np.amin(m[:,:,0:6],axis=(0,1))\n",
    "                print(paraMax,paraMin)\n",
    "                if s==0:\n",
    "                    self.xMax = paraMax\n",
    "                    self.xMin = paraMin\n",
    "                else:\n",
    "                    paraMax = self.xMax\n",
    "                    paraMin = self.xMin    \n",
    "                paraNorms = paraMax-paraMin\n",
    "                for ind,val in enumerate(paraNorms):\n",
    "                    if val > 0:\n",
    "                        m[:,:,ind] = np.divide(m[:,:,ind]-paraMin[ind],val)\n",
    "                return m        \n",
    "            \n",
    "            self.x_data = normalizeMatrix(self.x_data)\n",
    "            \n",
    "            if s==0:\n",
    "                self.yMax = np.amax(self.y_data)\n",
    "                self.yMin = np.amin(self.y_data)\n",
    "\n",
    "            #print(\"NOT normalizing encoded representation between 0 and 1\")\n",
    "            self.y_data = (self.y_data-self.yMin)/(self.yMax-self.yMin) \n",
    "            self.x_data[:,:,6:] = (self.x_data[:,:,6:]-self.yMin)/(self.yMax-self.yMin) \n",
    "            \n",
    "            print(self.xMax, self.xMin, self.yMax, self.yMin)\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(self.x_data[0:100,:,:].flatten(),'b.')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()            \n",
    "            plt.plot(self.y_data[0:100,:,:].flatten(),'r.')\n",
    "            plt.show()\n",
    "            \n",
    "            indices = np.arange(self.x_data.shape[0])       \n",
    "            random.seed(6)\n",
    "            random.shuffle(indices)\n",
    "            self.x_data = self.x_data[indices,:,:]\n",
    "            self.y_data = self.y_data[indices,:,:]\n",
    "            \n",
    "            if s == 0:\n",
    "                print(\"Training data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/LSTM/\" + sVar + \"/Dict_processed_data_MarsNew_2D_LSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_data_ae840_uniform.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data] , f, protocol=4)\n",
    "            if s == 1:\n",
    "                print(\"CV data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/LSTM/\" + sVar + \"/Dict_processed_data_MarsNew_2D_LSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_cv_ae840_uniform.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "            if s == 2:\n",
    "                print(\"Test data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/LSTM/\" + sVar + \"/Dict_processed_data_MarsNew_2D_LSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_test_ae840_uniform.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "\n",
    "        self.x = self.indexerDict[\"x\"]\n",
    "        self.y = self.indexerDict[\"y\"]\n",
    "        \n",
    "        profiles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.620808Z",
     "iopub.status.busy": "2021-03-13T00:37:52.620211Z",
     "iopub.status.idle": "2021-03-13T00:37:52.654290Z",
     "shell.execute_reply": "2021-03-13T00:37:52.654777Z"
    }
   },
   "outputs": [],
   "source": [
    "runThisCell=False\n",
    "if runThisCell:\n",
    "    numSims = [12000]\n",
    "    numChannels = 1\n",
    "    sVariables = ['T'] #, 'T']\n",
    "\n",
    "    dictToOpen = pathSave + \"processedDicts/Mars_new/T/Dict_NewMars_preProcessed_\" + str(numSims) + \"sims_0.98trPercent.txt\"\n",
    "    with open(dictToOpen, \"rb\") as myFile:\n",
    "        indexerDict = load(myFile)\n",
    "\n",
    "    for sVar in sVariables:\n",
    "        for n in numSims:\n",
    "            Dict_processed_data = {}\n",
    "            data = getDataLSTM_uniform(n, pathSave, numChannels, sVar, indexerDict)\n",
    "            Dict_processed_data['x'] = data.x \n",
    "            Dict_processed_data['y'] = data.y \n",
    "            Dict_processed_data['paraMax'] = data.xMax\n",
    "            Dict_processed_data['paraMin'] = data.xMin\n",
    "            Dict_processed_data['postEncodedOMax'] = data.yMax\n",
    "            Dict_processed_data['postEncodedOMin'] = data.yMin\n",
    "\n",
    "            with open(pathSave+ \"processedDicts/Mars_new/LSTM/T/Dict_processed_data_MarsNew_2D_LSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_uniform.txt\", \"wb\") as f:\n",
    "                dump(Dict_processed_data, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.678662Z",
     "iopub.status.busy": "2021-03-13T00:37:52.668000Z",
     "iopub.status.idle": "2021-03-13T00:37:52.715850Z",
     "shell.execute_reply": "2021-03-13T00:37:52.715264Z"
    }
   },
   "outputs": [],
   "source": [
    "class getDataMDN:\n",
    "    def __init__(self,_numSims, _pathSave, numChannels, sVar, indexerDict):\n",
    "        self.pathSave = _pathSave\n",
    "        self.numSims = _numSims  \n",
    "        self.sVar = sVar\n",
    "        self.indexerDict = indexerDict\n",
    "        \n",
    "        ae = ['T_Mars300_sims10k_f5A7s2_c3to24_tanh_l2reg_wlrsc', \"conv2d_6\", 840, [5,7,24], -11]   \n",
    "        autoencoder = load_model(self.pathSave + '/TrainedNetworks/autoencoder/' + ae[0] + '.hdf5')\n",
    "        autoencoder.summary()\n",
    "        encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(ae[1]).output)   \n",
    "        \n",
    "        self.oMax = indexerDict['pMax']\n",
    "        self.oMin = indexerDict['pMin']\n",
    "        \n",
    "        \n",
    "        maxTimeSteps = 0\n",
    "        for s in range(3):\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "            \n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "\n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "            \n",
    "            numSims = 0\n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    numSims += 1\n",
    "                    dictInd = val[0]\n",
    "                    timeSteps = 1 \n",
    "                else:\n",
    "                    timeSteps += 1\n",
    "                    if timeSteps > maxTimeSteps:\n",
    "                        maxTimeSteps = timeSteps\n",
    "        \n",
    "        for s in range(3):\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "\n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "            \n",
    "            numSims = 0            \n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    numSims += 1\n",
    "                    dictInd = val[0]\n",
    "                    timeSteps = 1 \n",
    "                else:\n",
    "                    timeSteps += 1\n",
    "                \n",
    "            print(numSims,maxTimeSteps)\n",
    "            \n",
    "            numStateVariables = numChannels #T, x, y\n",
    "            sizeStateVariable = self.indexerDict[\"x\"].shape[0] \n",
    "\n",
    "            numLayers = 302 #202\n",
    "            sizeEachLayer = int(sizeStateVariable/numLayers)\n",
    "            \n",
    "            totalSamples = len(indexerVec)\n",
    "            \n",
    "            self.y_data = np.zeros((numSims,5))\n",
    "            self.x_data = np.zeros((numSims,maxTimeSteps,ae[3][0]*ae[3][1]*ae[3][2]+2))\n",
    "                    \n",
    "            dictPrev = -1\n",
    "            dictInd = -1\n",
    "            for ind, i in enumerate(indexerVec):\n",
    "                if i[0] != dictPrev:\n",
    "                    dictToOpen = \"/plp_user/agar_si/gaia/python/Data/2D/Mars_new/\" + sVar + \"/savedDict_profiles_Mars_new_2D_\" \\\n",
    "                                                                            + sVar + \"_only_Dict\" + str(i[0]) + \".txt\"\n",
    "                    with open(dictToOpen, \"rb\") as myFile:\n",
    "                        profiles = load(myFile)\n",
    "                    dictPrev = i[0]\n",
    "                    timeStep = 0\n",
    "                    dictInd += 1\n",
    "                    \n",
    "                    self.y_data[dictInd,:] = [np.log10(float(profiles['InputValues'+ str(i[1])][1].split('=')[1])), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][5].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][6].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][7].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][8].split('=')[1].split(\"_\")[0].split(\".ini\")[0])]\n",
    "\n",
    "                field = profiles['Dict_2D_' + sVar + '_time_' + str(i[2]) + '_' + str(i[1])]\n",
    "                field.shape = (1,numLayers,sizeEachLayer,numStateVariables)\n",
    "\n",
    "                field = (field-self.oMin)/(self.oMax-self.oMin) \n",
    "                self.x_data[dictInd,timeStep,2:] = encoder.predict(field).flatten()\n",
    "                \n",
    "                if ind != len(indexerVec)-1 and indexerVec[ind][0] == indexerVec[ind+1][0]:\n",
    "                    t_delta = float(i[2])\n",
    "                    mask = 1\n",
    "                else:\n",
    "                    t_delta = 0\n",
    "                    mask = 0\n",
    "                    self.x_data[dictInd,timeStep+1:,0:2] = [mask, t_delta]\n",
    "                    self.x_data[dictInd,timeStep+1:,2:] = -1.1\n",
    "\n",
    "                self.x_data[dictInd,timeStep,0:2] = [mask, t_delta]\n",
    "                \n",
    "                timeStep += 1\n",
    "            \n",
    "            def normalizeMatrix(m):\n",
    "                paraMax = np.amax(m[:,:],axis=0)\n",
    "                paraMin = np.amin(m[:,:],axis=0)\n",
    "                if s==0:\n",
    "                    self.yMax = paraMax\n",
    "                    self.yMin = paraMin\n",
    "                else:\n",
    "                    paraMax = self.yMax\n",
    "                    paraMin = self.yMin    \n",
    "                paraNorms = paraMax-paraMin\n",
    "                print(paraNorms)\n",
    "                for ind,val in enumerate(paraNorms):\n",
    "                    if val > 0:\n",
    "                        m[:,ind] = np.divide(m[:,ind]-paraMin[ind],val)\n",
    "                return m        \n",
    "            \n",
    "            self.y_data = normalizeMatrix(self.y_data)\n",
    "            \n",
    "            if s==0:\n",
    "                self.xMax = np.amax(self.x_data[:,:,2:])\n",
    "                self.xMin = np.amin(self.x_data[:,:,2:])\n",
    "                self.deltatMax = np.amax(self.x_data[:,:,1])\n",
    "                self.deltatMin = np.amin(self.x_data[:,:,1])\n",
    "\n",
    "            self.x_data[:,:,2:] = (self.x_data[:,:,2:]-self.xMin)/(self.xMax-self.xMin) \n",
    "            self.x_data[:,:,1] = (self.x_data[:,:,1]-self.deltatMin)/(self.deltatMax-self.deltatMin) \n",
    "            \n",
    "            print(self.xMax, self.xMin, self.yMax, self.yMin, self.deltatMax, self.deltatMin)\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(self.x_data[0:100,:,:].flatten(),'b.')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()            \n",
    "            plt.plot(self.y_data[0:100,:].flatten(),'r.')\n",
    "            plt.show()\n",
    "            \n",
    "            indices = np.arange(self.x_data.shape[0])       \n",
    "            random.seed(6)\n",
    "            random.shuffle(indices)\n",
    "            self.x_data = self.x_data[indices,:,:]\n",
    "            self.y_data = self.y_data[indices,:]\n",
    "            \n",
    "            if s == 0:\n",
    "                print(\"Training data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/MDN/\" + sVar + \"/Dict_processed_data_MarsNew_2D_MDN_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_data.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data] , f, protocol=4)\n",
    "            if s == 1:\n",
    "                print(\"CV data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/MDN/\" + sVar + \"/Dict_processed_data_MarsNew_2D_MDN_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_cv.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "            if s == 2:\n",
    "                print(\"Test data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/MDN/\" + sVar + \"/Dict_processed_data_MarsNew_2D_MDN_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_x_test.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "\n",
    "        self.x = self.indexerDict[\"x\"]\n",
    "        self.y = self.indexerDict[\"y\"]\n",
    "        \n",
    "        profiles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.720986Z",
     "iopub.status.busy": "2021-03-13T00:37:52.720424Z",
     "iopub.status.idle": "2021-03-13T00:37:52.754219Z",
     "shell.execute_reply": "2021-03-13T00:37:52.754718Z"
    }
   },
   "outputs": [],
   "source": [
    "runThisCell=False\n",
    "if runThisCell:\n",
    "    numSims = [12000]\n",
    "    numChannels = 1\n",
    "    sVariables = ['T'] #, 'T']\n",
    "\n",
    "    dictToOpen = pathSave + \"processedDicts/Mars_new/T/Dict_NewMars_preProcessed_\" + str(numSims) + \"sims_0.9trPercent.txt\"\n",
    "    with open(dictToOpen, \"rb\") as myFile:\n",
    "        indexerDict = load(myFile)\n",
    "\n",
    "    for sVar in sVariables:\n",
    "        for n in numSims:\n",
    "            Dict_processed_data = {}\n",
    "            data = getDataMDN(n, pathSave, numChannels, sVar, indexerDict)\n",
    "            Dict_processed_data['x'] = data.x \n",
    "            Dict_processed_data['y'] = data.y \n",
    "            Dict_processed_data['paraMax'] = data.xMax\n",
    "            Dict_processed_data['paraMin'] = data.xMin\n",
    "            Dict_processed_data['postEncodedOMax'] = data.yMax\n",
    "            Dict_processed_data['postEncodedOMin'] = data.yMin\n",
    "\n",
    "            with open(pathSave+ \"processedDicts/Mars_new/MDN/T/Dict_processed_data_MarsNew_2D_ConvLSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel.txt\", \"wb\") as f:\n",
    "                dump(Dict_processed_data, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.768448Z",
     "iopub.status.busy": "2021-03-13T00:37:52.761997Z",
     "iopub.status.idle": "2021-03-13T00:37:52.816830Z",
     "shell.execute_reply": "2021-03-13T00:37:52.816223Z"
    }
   },
   "outputs": [],
   "source": [
    "class getDataMDN_convlstm:\n",
    "    def __init__(self,_numSims, _pathSave, numChannels, sVar, indexerDict):\n",
    "        self.pathSave = _pathSave\n",
    "        self.numSims = _numSims  \n",
    "        self.sVar = sVar\n",
    "        self.indexerDict = indexerDict\n",
    "        \n",
    "        ae = ['T_Mars300_sims10k_f5A7s2_c3to24_tanh_l2reg_wlrsc', \"conv2d_6\", 840, [5,7,24], -11]   \n",
    "        autoencoder = load_model(self.pathSave + '/TrainedNetworks/autoencoder/' + ae[0] + '.hdf5')\n",
    "        autoencoder.summary()\n",
    "        encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(ae[1]).output)   \n",
    "        \n",
    "        self.oMax = indexerDict['pMax']\n",
    "        self.oMin = indexerDict['pMin']\n",
    "        \n",
    "        \n",
    "        maxTimeSteps = 0\n",
    "        for s in range(3):\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "            \n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "\n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "            \n",
    "            numSims = 0\n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    numSims += 1\n",
    "                    dictInd = val[0]\n",
    "                    timeSteps = 1 \n",
    "                else:\n",
    "                    timeSteps += 1\n",
    "                    if timeSteps > maxTimeSteps:\n",
    "                        maxTimeSteps = timeSteps\n",
    "\n",
    "        for s in range(3):\n",
    "            if s==0:\n",
    "                stringInt = \"train\"\n",
    "            elif s==1:\n",
    "                stringInt = \"cv\"\n",
    "            else:\n",
    "                stringInt = \"test\"\n",
    "            \n",
    "            indexerVec = self.indexerDict[\"Indexer_\" + stringInt]\n",
    "\n",
    "            indexerVec = [tuple(s) for s in indexerVec]\n",
    "            dtype = [('dict', int), ('useless', int), ('time', float)]\n",
    "            indexerVec = np.array(indexerVec, dtype=dtype)\n",
    "            indexerVec = np.sort(indexerVec, order=['dict', 'time']) \n",
    "            indexerVec = indexerVec.tolist()\n",
    "            \n",
    "            numSims = -1\n",
    "            simList = []\n",
    "            timeList = []\n",
    "            dictInd = -1\n",
    "            for ind,val in enumerate(indexerVec):\n",
    "                if val[0] != dictInd:\n",
    "                    numSims += 1\n",
    "                    if dictInd != -1:\n",
    "                        timeList.append(innertimeList)\n",
    "                        simList.append(dictInd)\n",
    "                    dictInd = val[0]\n",
    "                    innertimeList = []\n",
    "                else:\n",
    "                    innertimeList.append(val[2])\n",
    "               \n",
    "            print(numSims,maxTimeSteps)\n",
    "            \n",
    "            numStateVariables = numChannels #T, x, y\n",
    "            sizeStateVariable = self.indexerDict[\"x\"].shape[0] \n",
    "\n",
    "            numLayers = 302 #202\n",
    "            sizeEachLayer = int(sizeStateVariable/numLayers)\n",
    "            \n",
    "            maxTimeSteps = 50\n",
    "            spacedTimeVec = np.linspace(0,0.04913813148788927,maxTimeSteps)\n",
    "            \n",
    "            indexerVec = []\n",
    "            tol = 1e-3\n",
    "            \n",
    "            for ind, sim in enumerate(simList):\n",
    "                innerTimeList = []\n",
    "                for time in spacedTimeVec:\n",
    "                    whereVec = np.where((time>=np.asarray(timeList[ind])-tol) & (time<=np.asarray(timeList[ind])+tol))[0]\n",
    "                    if len(whereVec) > 0:\n",
    "                        indexerVec.append([sim,0,timeList[ind][whereVec[-1]]])\n",
    "                    else:\n",
    "                        indexerVec.append([sim,0,-1])\n",
    "            \n",
    "            self.y_data = np.zeros((numSims,5))\n",
    "            self.x_data = np.zeros((numSims,maxTimeSteps,ae[3][0],ae[3][1],ae[3][2]))\n",
    "                    \n",
    "            dictPrev = -1\n",
    "            dictInd = -1            \n",
    "            for ind, i in enumerate(indexerVec):\n",
    "                if i[0] != dictPrev:\n",
    "                    dictToOpen = \"/plp_user/agar_si/gaia/python/Data/2D/Mars_new/\" + sVar + \"/savedDict_profiles_Mars_new_2D_\" \\\n",
    "                                                                            + sVar + \"_only_Dict\" + str(i[0]) + \".txt\"\n",
    "                    with open(dictToOpen, \"rb\") as myFile:\n",
    "                        profiles = load(myFile)\n",
    "                    \n",
    "                   \n",
    "                    dictPrev = i[0]\n",
    "                    timeStep = 0\n",
    "                    dictInd += 1\n",
    "                    \n",
    "                    self.y_data[dictInd,:] = [np.log10(float(profiles['InputValues'+ str(i[1])][1].split('=')[1])), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][5].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][6].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][7].split('=')[1]), \\\n",
    "                                float(profiles['InputValues'+ str(i[1])][8].split('=')[1].split(\"_\")[0].split(\".ini\")[0])]\n",
    "\n",
    "                if i[2] != -1:\n",
    "                    field = profiles['Dict_2D_' + sVar + '_time_' + str(i[2]) + '_' + str(i[1])]\n",
    "                    field.shape = (1,numLayers,sizeEachLayer,numStateVariables)\n",
    "\n",
    "                    field = (field-self.oMin)/(self.oMax-self.oMin) \n",
    "                    self.x_data[dictInd,timeStep,:,:,:] = encoder.predict(field)\n",
    "                else:\n",
    "                    self.x_data[dictInd,timeStep,:,:,:] = -1.1\n",
    "                \n",
    "                timeStep += 1\n",
    "            \n",
    "            def normalizeMatrix(m):\n",
    "                paraMax = np.amax(m[:,:],axis=0)\n",
    "                paraMin = np.amin(m[:,:],axis=0)\n",
    "                print(paraMin)\n",
    "                if s==0:\n",
    "                    self.yMax = paraMax\n",
    "                    self.yMin = paraMin\n",
    "                else:\n",
    "                    paraMax = self.yMax\n",
    "                    paraMin = self.yMin    \n",
    "                paraNorms = paraMax-paraMin\n",
    "                for ind,val in enumerate(paraNorms):\n",
    "                    if val > 0:\n",
    "                        m[:,ind] = np.divide(m[:,ind]-paraMin[ind],val)\n",
    "                return m        \n",
    "            \n",
    "            self.y_data = normalizeMatrix(self.y_data)\n",
    "            \n",
    "            if s==0:\n",
    "                self.xMax = np.amax(self.x_data[:,:,:,:,:])\n",
    "                self.xMin = np.amin(self.x_data[:,:,:,:,:])\n",
    "\n",
    "            self.x_data[:,:,:,:,:] = (self.x_data[:,:,:,:,:]-self.xMin)/(self.xMax-self.xMin) \n",
    "            \n",
    "            print(self.xMax, self.xMin, self.yMax, self.yMin) #, self.deltatMax, self.deltatMin)\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(self.x_data[0:100,:,:,:,:].flatten(),'b.')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure()            \n",
    "            plt.plot(self.y_data[0:100,:].flatten(),'r.')\n",
    "            plt.show()\n",
    "            \n",
    "            indices = np.arange(self.x_data.shape[0])       \n",
    "            random.seed(6)\n",
    "            random.shuffle(indices)\n",
    "            self.x_data = self.x_data[indices,:,:,:,:]\n",
    "            self.y_data = self.y_data[indices,:]\n",
    "            \n",
    "            if s == 0:\n",
    "                print(\"Training data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/MDN/\" + sVar + \"/Dict_processed_data_MarsNew_2D_MDNConvLSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae840_x_data.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data] , f, protocol=4)\n",
    "            if s == 1:\n",
    "                print(\"CV data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/MDN/\" + sVar + \"/Dict_processed_data_MarsNew_2D_MDNConvLSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae840_x_cv.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "            if s == 2:\n",
    "                print(\"Test data: \" + str(self.x_data.shape) + str(self.y_data.shape))\n",
    "                with open(pathSave+ \"processedDicts/Mars_new/MDN/\" + sVar + \"/Dict_processed_data_MarsNew_2D_MDNConvLSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae840_x_test.txt\", \"wb\") as f:\n",
    "                    dump([self.x_data,self.y_data], f, protocol=4)\n",
    "\n",
    "        self.x = self.indexerDict[\"x\"]\n",
    "        self.y = self.indexerDict[\"y\"]\n",
    "        \n",
    "        profiles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-13T00:37:52.822076Z",
     "iopub.status.busy": "2021-03-13T00:37:52.821510Z",
     "iopub.status.idle": "2021-03-13T00:37:52.855654Z",
     "shell.execute_reply": "2021-03-13T00:37:52.855060Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "runThisCell=False\n",
    "if runThisCell:\n",
    "    numSims = [12000]\n",
    "    numChannels = 1\n",
    "    sVariables = ['T'] #, 'T']\n",
    "\n",
    "    dictToOpen = pathSave + \"processedDicts/Mars_new/T/Dict_NewMars_preProcessed_\" + str(numSims) + \"sims_0.9trPercent.txt\"\n",
    "    with open(dictToOpen, \"rb\") as myFile:\n",
    "        indexerDict = load(myFile)\n",
    "\n",
    "    for sVar in sVariables:\n",
    "        for n in numSims:\n",
    "            Dict_processed_data = {}\n",
    "            data = getDataMDN_convlstm(n, pathSave, numChannels, sVar, indexerDict)\n",
    "            Dict_processed_data['x'] = data.x \n",
    "            Dict_processed_data['y'] = data.y \n",
    "            Dict_processed_data['paraMax'] = data.xMax\n",
    "            Dict_processed_data['paraMin'] = data.xMin\n",
    "            Dict_processed_data['postEncodedOMax'] = data.yMax\n",
    "            Dict_processed_data['postEncodedOMin'] = data.yMin\n",
    "\n",
    "            with open(pathSave+ \"processedDicts/Mars_new/MDN/T/Dict_processed_data_MarsNew_2D_MDNConvLSTM_\" + sVar + '_' + str(numSims) + \"sims_\" + str(numChannels) + \"channel_ae840.txt\", \"wb\") as f:\n",
    "                dump(Dict_processed_data, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
